\documentclass[12pt,twoside]{report}
\usepackage{tikz-cd}

\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage{listings}

\RequirePackage{filecontents}
\begin{filecontents*}{\jobname.bib}
@Book{Goossens,
  author    = {Goossens, Michel and Mittelbach, Frank and 
               Samarin, Alexander},
  title     = {The LaTeX Companion},
  edition   = {1},
  publisher = {Addison-Wesley},
  location  = {Reading, Mass.},
  year      = {1994},
}
@Book{adams,
  title     = {The Restaurant at the End of the Universe},
  author    = {Douglas Adams},
  series    = {The Hitchhiker's Guide to the Galaxy},
  publisher = {Pan Macmillan},
  year      = {1980},
}
\end{filecontents*}


\usepackage{tikz}
\usetikzlibrary{arrows.meta, graphs, graphs.standard}
\tikzset{> = Stealth[round]}



\usepackage{a4wide}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[inline]{asymptote}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{xcolor}
\graphicspath{ {images/} }
\usepackage{caption}
\usepackage{tikz}
\usepackage{yfonts}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}
\usepackage{fancyhdr}
\usepackage{listings}
\pagestyle{fancy}

\usepackage{biblatex2bibitem}
\usepackage{biblatex}

\fancyhf{}
\fancyhead[RE,LO]{\rightmark}
\fancyfoot[RE,LO]{\thepage}
\usepackage[font=small,labelfont=bf]{caption}

\newcommand{\Rlogo}{\protect\includegraphics[height=1.8ex,keepaspectratio]{Rlogo.png}}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\usepackage{hyperref}
\usepackage{float}
\restylefloat{table}
\usepackage{algorithm2e}
\RestyleAlgo{ruled}

\definecolor{mypink1}{rgb}{0.858, 0.188, 0.478}
\definecolor{mypink2}{RGB}{219, 48, 122}
\definecolor{mypink3}{cmyk}{0, 0.7808, 0.4429, 0.1412}
\definecolor{mygray}{gray}{0.6}

\usepackage[ngerman]{babel}%added
\usepackage[backend=biber,style=apa]{biblatex}%mod.
\usepackage[german=quotes]{csquotes}%mod.
\usepackage[locale=US]{siunitx}
\DeclareLanguageMapping{ngerman}{ngerman-apa}
\addbibresource{Literatur.bib}


\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{lineno}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{url}
\numberwithin{equation}{chapter}

\usepackage[style=authoryear,sorting=none]{biblatex}








\begin{document}
\noindent 
\begin{center}
\textbf{ARCHITECTURAL DESIGNS FOR OMNI-INTELLIGENCE}
\newline
[ALI NAJIB] CORE FILE
\end{center}

\noindent \textbf{Project Essay, Research Proposal. (Formalize later. Extend later.)} 

\textit{Outline of Continuous Pursuit gains from modern commentary, as much of legacy Outline has been written approximately one year ago. Much of the available script is a literal copy of legacy Outline, and so has a rewriting of Outline been planned. This is a task laborious, but the Outline specified in  \textcolor{blue}{ https://github.com/Ali-Al-Habsyi} serves to found all that the below is part of, and guarantees monetary flow. The implementation of this Outline is where we start. Pursue extension by generalization towards automation of conceptualization, inspiration at the largest scale, from observations at the sharpest level and of the largest scale; a grand theorizer, a grand observer. See file-bibliography for definitions. }



\textbf{Substance first. Rewrite later.}
Convolutional Neural Network construction by Deep Reinforcement Learning and Chip-placement by Deep Reinforcement Learning are one-to-one translatable. Setting-dependency lies in variation by specialisation of observer cognition. AlphaFold's architecture serves as one example for a specialized observer. Graphcast is another example. AlphaFold can be AlphaChip-assembled, given the present elements in AlphaFold. AlphaTensor and AlphaDev serve as constructors, and both have indeed been written in the framework of Deep Reinforcement Learning. The same holds for MuZero. The crux lies in the architecture of observation and its transformability. The elements of the architecture are setting-dependent, and the congnition of the observer varies by specialization.

Notably, two-dimensional convolutional neural-network construction and chip placement with Deep Reinforcement Learning are one-to-one translatable, and for Navigation Control can we use a secondary, iteratively transformable, neural-network that takes inputs on iteratively transforming primary neural-networks. The essence of any form of application lies in their respective domain-constants, translatable to learning architecture. (Examples are to be found in Hubble's Law's cosmological constants for pre-filtering red-shift and blue-shift as utilized in the LOFAR-Sky Survey.)

Obstacle detection and circumvention in motion through space of observables is computable and simulable in the framework of Deep Reinforcement Learning. Deep Sea deployment of Deep Sea Mining Vehicle is reconstructible in traversal over every other platform if given the required sensors. (In DSMV: $\delta_{\text{tar}}, \delta_{OLS}, i_{RI}, \dot{w}_{b}$, including distance to obstacle). DSMV target path Navigation Control Learnable for DSMV realization, and in turn Drone realization, or Land Crawler, Mars Rover realization.

Path-planning in obstacle circumvention feasible by incorporation of setting specific obstacle-type in pseudospectral control for unknown ocean currents, generalizable to air currents. 
Navigation control manual construction potential starting point in path planning for underactuated vehicle or roamer by sensor-induced state-representation restricted to vision captured by installed visual system. (Complication in state-representation must correspond to complexity in area of deployment.)

Encoded observable information extraction from information-technology systems such as HIS, LIS, PACS, EMR, existing monitoring for utility traces. Mass: virtuality/reality synchronization for simulation pre-deploy. Examples for virtual spaces of observables include Financial Time Series Databases (Large Scale Cyberspace). Neuro-image by IC-scan in higher-dimensions. Traversal over Internet of Things, all traversable by method of traversal in physical space, by translation brain-network topology representation in structure, and dynamics allows for a learner's motion, or traversal, over the brain, in pursuit, e.g., of an appropriate Network hyperparametrization (as to fit the network's orgin in biological cognition). Motion on a guided path would e.g. occur by deep reinforcement Learning. This, indeed, lays bears the possiblity of mathematical optimization over all of a neural-network's parameters, leaving no room for human intuition in tweaking hyperparameters. Applications in financial time-series owe their origin in databases on elementary trade.


% ------------------------------- Delayed addition -------------------------------

\noindent \textbf{Delayed addition.} 
Correctness check and efficiency consideration by fixing constant nature, such as constant Hubble's Law. For focus of learning on the unknown, comparable to temporal basis for time-series. Detection and capture vision dependent on nature detectee and translatable to architecture learning. Navigation through observables may be translated to plain terms as 'knowing where to look over all things that can be seen', 'zooming in', 'varying telescope orientation', 'shifting microscope sample'. Navigation through observables is an observable learnable by, e.g., RNN constructed by Chip placement. Note that navigation through observables assumes inherent limitation in range observer vision. Necessity of navigation through observables assumes excessive abundance of observables for preconceptualization by observation of one vicinity. Autonomous vehicle localization and map-matching serve to bear hints towards heuristic boundaries for navigation towards, or over, preconceptualization. (Momentary example is based on extent of conceptualization, and differences therein over continuous points in space of observables). Higher-orbit or deep-space or deep-sea (Space/Ocean)craft navigation policy towards guided exploration or faring towards body, based on X-ray pulsar capture, may bear hints towards dynamics of observer over space of observables, the latter yielding a similar extent of mystery, and encoding after observation, by conceptualization. \\

\noindent \textbf{First Preliminary Network Architectural Design.} Sources include DeepMind's AlphaFold, AplhaStar, AlphaChip, AlphaGlobe, AlphaDev, AlphaMolstruct, AlphaProteome, Graphcast. The below outlines a blueprint of Network Architectural Design. \footnotesize
\begin{align*}
&\overset{**}{\boxed{\text{Reality}}}
 \overset{(1) }{\leftrightarrows} 
 \underset{\updownarrows \dots}{\boxed{\text{cross-DL}}}
\overset{(2)}{\leftrightarrows } 
\underset{\updownarrows \dots}{\boxed{\text{Learner-calibration DRL} }}
\overset{(3)}{\leftrightarrows } 
\underset{\updownarrows \dots}{\boxed{
\begin{array}{rcl}
&\text{
Scoper of} \\
&\text{
Parametrization DRL }
\end{array}
}}
\overset{(3)}{\leftrightarrows } 
\dots \\
&\boxed{\text{(simulated)  World}} \overset{(4) }{\leftrightarrows} 
\overset{**}{\boxed{\text{Walker DRL (\textdagger)}}}
\overset{(5) }{\leftrightarrows} 
\boxed{\text{Walker-calibration DRL}} \overset{(6)}{\leftrightarrows } 
\dots. \\
\end{align*}
%\end{center}
\normalsize
A special case of the frame
\begin{align*}
& \boxed{\text{Reality}} \overset{(7) }{\leftrightarrows} \boxed{\text{DGN}}. \\
\end{align*}
%\end{center}
All are interconnected.
\normalsize
\begin{align*}
\overset{***}{\boxed{\text{
Reality}}}
&\overset{(8)}{\leftrightarrows }
\overset{****}{\boxed{\text{
Cross-DL}}} \\
\overset{***}{\boxed{\text{
\begin{array}{rcl}
&\text{
Globe Maps} \\
&\text{
PcChips } \\
&\text{
MolStruct } \\
&\text{
\tilde \geq Conduct Cross-observation }
\end{array}
}}}
 &\overset{(9)}{\leftrightarrows } \overset{****}{\boxed{
\begin{array}{rcl}
&\text{
Graphcast} \\
&\text{
AlphaChip } \\
&\text{
AlphaProteome } \\
&\text{
CrossDL } \\
\end{array}
}}
\end{align*}

\noindent (Sources include DeepMind's AlphaFolds, AlphaStar, AlphaChip, AlphaGlobe, AlphaDev, AlphaMolstruct, AlphaProteome, Graphcast. Arrow directed up and down, to be integrated, indicate extension of representation and correction higher degrees of extensivity). For all the other symbols follows now a legend. \\

\noindent \textbf{First Preliminary Network Architectural Design.} (Legend One) 
\begin{enumerate}
  \item $\rightarrow$: interpretation, $\leftarrow$: representation.
  \item $\rightarrow$: interpretation, $\leftarrow$: representation. \textit{AlphaChip network-construction defines this link.}
  \item $\rightarrow$: interpretation, $\leftarrow$: representation. \textit{AlphaChip network-construction defines this link.}
   \item $\rightarrow$: interpretation, $\leftarrow$: navigation. \textit{AlphaChip network-construction defines this link.}
\item $\rightarrow$: interpretation, $\leftarrow$: representation. 
\item $\rightarrow$: inspection, $\leftarrow$: correction. \textit{AlphaChip network-construction defines this link.}
\item $\rightarrow$: inspection, $\leftarrow$: correction. \textit{AlphaChip network-construction defines this link.} 
\item $\rightarrow$: interpretation, $\leftarrow$: representation. 
\end{enumerate}
\textbf{(**).} The two boxes marked by (**) are interlinked by navigation of [Walker-DRL] over [Reality] and interpretation of [Reality] by [Walker-DRL]. \\
\textbf{(***).} The two boxes marked by (***) are interlinked by representation of [Reality] by [GlobeMaps, PcChips, MolStruct, ConductCrossObservation]. \\
\textbf{(****).} The two boxes marked by (****) are interlinked by representation of [Cross-DL] by [GraphCast, AlphaChip, AlphaProteome, CrossDL]. \\



\noindent \textbf{First Algorithmic Set-up.} [First Algorithmic Set-up; First expression in terms of popular notions]
\begin{enumerate}
  \item Conv3D network assembly in stages defined by distance between network specifications with assembly procedure by AlphaChip chip-placement DRL. Implementation follows from a specification of all the attributes adhering to a DRL-agent that performs network-construction. Demonstration follows from AlphaChip.

  \item 

  Setting-specificity might bring rise to inputs that are no outputs of vision. Network assembly procedure possible by Chip-placement DRL if distances are defined via setting specificity. (Difficult example is AlphaFold assembled by AlphaChip).
  
\item Conv3D Network Chip placement assembly possible by calibrating on one time-varying variable, see (1). For (online) deployment and assembly of Conv3D Network, attach Conv3D Network to RNN-Network as, e.g., specified in [DLB], Figure 10.4.2.



\item 


Deep Reinforcement Learning: Area of deployment, either in virtual or physical space, e.g. IoT or historical records, or some region. Virtual space can simulate physical space (use, e.g. Unity or Unreal Engine or SimuLab).

States may be defined by oculary picture(s), or oculary location and orientation, and actions may be defined by oculary-movement. Rewards are derived from Algorithm (3) installed observer.

Training may be performed by PPO or SARSA($\lambda$) both being online reinforcement learning algorithms. Training by performing any offline reinforcement learning algorithm will take much time before convergence due to space's breadth.


\item 

Hyperparametric consideration: area of deployment must be known for definition states under algorithm 4, this is true for simulated space, otherwise area must be downloaded by e.g. agent's random walk, as initialization stage, or a new reinforcement learning algorithm. Under Algorithm 3, value for variable in calibration is set under oculary's initial position, e.g., or is searched by orientation, and choice of oculary influences output algorithm 1.

\end{enumerate}

\noindent \textbf{Pinpointing the problems in existing techniques in Machine Learning.} Consider $\mathcal{D}_{f}$, residing in an environment and bearing phenomena about which one might have some preconception, but little notion of theory towards a representable property of interest. A first problem is to find the representation, and this might in turn occur by a first application of $[\text{CrossDL}] \rightleftharpoons[\text{WalkerDRL}]$ equipped only with Vision in an area near $\mathcal{D}_{f}$, either in a non-virtual or virtual sense. Afterwards, equipped with representations of $\mathcal{D}_{f}$, one may re-deploy the device for learnability of $\mathcal{D}_{f}$. Stage 1 deployment occurs by observation of the phenomena in a sense that need not yield more clarification than initial encounter, i.e. first contact, towards $\mathcal{D}_{f}$, that at this point might be known by any data type, e.g. that which made the researcher declare initial encounter by Vision.

If the representation resulting from the initial Vision encounter succeeds in representing the property of interest, the encounter encoded by $\mathcal{D}_{f}$ may be used for stage 2 deployment, in which case stage 1 would result in the representation $\mathcal{D}_{f}$ for encounter $\mathcal{D}_{f}$


Learning may occur online or offline, and the assumption of data availability is essential. One has some preconception of the relation between target and relevant features, and these features have been selected beforehand therefore. Any preconception may occur on the basis of premonition or established theory. Learning the environment happens thereafter, on the basis of the latter isolated environment. \\

\noindent \textbf{Representing any target, or feature, or environment (One).}
Many problems in conventional machine learning seek to learn for predictability, or explainability. There are phenomena that, however, may barely be observable, or encodings that may cause information loss to great extents. One may not observe much latency in a defined numeric feature that does not in the least represent $\mathcal{D}_{f}$, the encounter of interest.


To learn an environment is to enable accurate generation of extensions of the environment, in the broadest sense that an environment reveals itself. The matter of feature-selection occurs itself by learning, and so does the matter of feature-identification occur itself by learning (from vision-lens encoded input). The same would hold for any target, whence more information than mere binary encoding represents an encounter of interest. \\

\noindent \textbf{Scale One.} It is reasonable to assume that data-entanglement with $\mathcal{D}_{f}$ occurs near $\mathcal{D}_{f}$, and while this is of interest for small-scale, conventional learning, one may be interested in relations that exist over large distances in space, either temporal, physical or virtual. One far-fetched device that would learn and capture such an entanglement is a satellite equipped with [CrossDL] for relations that attract [WalkerDRL]. The idea, again is to specify an inquiry from which some representable target of interest follows, to be encoded in an alternative reality to which we have access, so that the representable target may be reconstructed and extended. The target may be the means by which first encounter has occurred, and may entail an entire physical environment. 
The device, be it an orbital satellite or a drone that explores dataspace by traversing planetary orbit and performing areal rend-a-vouz respectively. As one would seek preconception on a target, would the device traverse an area, or a zone, to mine for data-entanglement. 
The problem resides mainly in feature-identification from, e.g., vision by lens, and to define the states in some state-space so as to optimize environment generation. In summary, the device is the learner as a whole, a completion. We construct and configure the device by specifying the objective, the target. The device subsequently observes, learns, observes, learns, \dots., until reached terminal criterion. \\


\noindent \textbf{Feature-collection and identification One.} \textit{State-specification.} Pattern happens by preconception of environment of deployment to be automated by $[\text{WalkerDRL}]$. Feature-collection depends on device of observation, thermolens given spectral features that might relate to $\mathcal{D}_{f}$, and thermolens might be captured by HDR lens. Feature-collection by VisionCNN  captures many features that are likely entangled with $\mathcal{D}_{f}$, captured by only a Vision-lens. (The sense of 'lens' is highly general: for a start can lenses be virtual data-collectors over some virtual DataSpace).
Feature-collection depends on device of observation. Feature-collection is possible via indicator-measurement, as, e.g., through a thermostat, or a level of chemical matter extraction as measured by $\text{pH}$. Again, both the latter phenomena may be captured by a Vision-lens.
Feature-Collection in sound may be processed by CNN by sound-representation in audiogram. Other ways of incorporation towards state $s$ are to be considered for reasons of efficiency. (This form of data might rarely be useful.) The problem of reward-specification reads as $[\text{CrossDL}]_{\mathcal{D}_{f}}(\mathcal{D}_{\text{data}}) \overset{L^{'}}{\rightarrow} (L_{(i_{1},\dots, i_{n})})_{\text{Combper}(i_{1},\dots,i_{n})}$. For momentary convenient simplicity, consider the problem $[\text{CrossDL}]_{\mathcal{D}_{f}} \overset{L}{\rightarrow} (L_{ij})_{i,j \in \{ 1, \dots,n\}}$. The function $L$ is defined and results in $(L_{ij})_{i,j \in \{ 1n\dots,n\}}$ with magnitudes depending on input $[\text{CrossDL}]_{\mathcal{D}_{f}} (\mathcal{D}_{\text{data}})$. The component $[\text{CrossDL}]_{\mathcal{D}_{f}}$ depends on finding good representations of $\mathcal{D}_{f}$. Specification of $\mathcal{D}_{f}$ is essential, and predictability, or computability, or learnability depends on this representation. \\

\noindent \textbf{First Preliminary Elementary Specification.} The below have been specified under the assumption of the reader's slight familiarity with reinforcement learning. We may invoke references for bringing full algorithmic detail, laying waste to all dependency in pre-defined function. The reader might derive great comfort from mathematical proof distilled to first principles, and so shall we provide some. Formulation [WalkerDRL] in full extension. Difficulties lie in environment-specification in terms of the space defined by state-action-reward tuples $(s, a, r)$. \\

%\noindent \textbf{First Preliminary Algorithm.} First Preliminary Algorithm. %\begin{align*}
%(s', a', r') &\leftarrow \text{Environment Learning: Exploring Starts (e.g.), vision-CNN/RNN} \\
%(\pi_{A}, \pi_{C}) &\leftarrow \text{Navigation Learning: PPO (e.g.)} \\
%\text{Gen.} &\leftarrow \text{Generation}(\pi_{A}, \pi_{C}, [\text{CrossDL}]_{\mathcal{D}_{f}})) \,\,\,\square  
%\end{align*}
\noindent \textbf{First Preliminary Algorithm.} First Preliminary Algorithm.
\begin{algorithm}
\caption{First Preliminary Algorithm.}\label{alg:cap}
\begin{algorithmic}
$(s', a', r') \leftarrow \text{Environment Learning: Exploring Starts (e.g.)}$ \\
$(\pi_{A}, \pi_{C}) \leftarrow \text{Navigation Learning: PPO (e.g.)}$ \\
$\text{Gen.} \leftarrow \text{Generation}(\pi_{A}, \pi_{C}, [\text{CrossDL}]_{\mathcal{D}_{f}})) \,\,\,\square  $
\end{algorithmic}
\end{algorithm}


%\noindent \textbf{Second Preliminary Algorithm.} Iterate until the threshold %criterion or (near convergence)
%\begin{align*}
%(s^{2}, a^{2}, r^{2}) &\leftarrow \text{Environment Learning: Exploring starts %over} \{s^{2}, a^{2} \}, \text{vision-CNN/RNN} \\
%\{ \pi \} &\leftarrow \text{PPO or DQN} \\
%\text{Gen.} &\leftarrow \text{Generation}(\pi, [CrossDL]_{\mathcal{D}_{f}}) \\
%&\text{Re-define} \,\,\{ s^{2}, a^{2}, r^{2}\} \,\,
%\text{and vary over loss-configuration.} \,\,\,
%\square 
%\end{align*}


\noindent \textbf{Second Preliminary Algorithm.} Second Preliminary Algorithm.
\begin{algorithm}
\caption{Second Preliminary Algorithm.}\label{alg:cap}
\begin{algorithmic}
$(s^{2}, a^{2}, r^{2}) &\leftarrow \text{Environment Learning: Exploring starts over} \{s^{2}, a^{2} \}$ \\
$\text{Gen.} &\leftarrow \text{Generation}(\pi, [CrossDL]_{\mathcal{D}_{f}})$ \\
$\text{Gen.} \leftarrow \text{Generation}(\pi_{A}, \pi_{C}, [\text{CrossDL}]_{\mathcal{D}_{f}})) \,\,\, $
$\text{Re-define} \,\,\{ s^{2}, a^{2}, r^{2}\} \,\,
\text{and vary over loss-configuration.} \,\,\, \square $
\end{algorithmic}
\end{algorithm}





\noindent \textbf{Third Preliminary Algorithm.}
Reinforcement Learning Configuror with state-space $(s^{3}, a^{3}$, loss-configuration($r$)), first lower-order reinforcement learner hyperparameters, first lower-order reinforcement learner action space). One such a first lower-order learner by reinforcement is [NavigatorDRL]. To learn the proper configuration of [NavigationDRL] deploy the following and iterate until threshold criterion or (near) convergence.
%\begin{align*}
%&\text{Environment learning:} \, \{s,a,r\}\overset{ \text{output}}{\leftarrow} %\text{Exploring starts over} \,\{ s, a, \text{loss-configuration}(r, L)\} . \\
%&\text{Navigation Learning:}\,\, \{ \pi \} \leftarrow \text{PPO e.g.}\\ 
%&\text{Target Generation}  \leftarrow \text{Gen.} \,\,\,\,\,\,\,\,  \\ 
%&\{s, a, r\} \leftarrow \text{Environment Learning: ExploringStarts (e.g.), %vision-CNN/RNN} \\
%&(\pi_{A}, \pi_{C}) \leftarrow \text{Navigation Learning: PPO (e.g.)} \\
%&\text{Gen.} \leftarrow \text{Generation}(\pi_{A}, \pi_{C}, %[\text{CrossDL}]_{\mathcal{D}_{f}}).  \,\,\,\,\,\,\,\square
%\end{align*}

\begin{algorithm}
\caption{Third Preliminary Algorithm.}\label{alg:cap}
\begin{algorithmic}
$\text{Environment learning:} \, \{s,a,r\}\overset{ \text{output}}{\leftarrow} \,\,\,\,\text{Exploring starts over} \,\{ s, a, \text{loss-configuration}(r, L)\}$. \\
$\text{Navigation Learning:}\,\, \{ \pi \} \leftarrow \text{PPO e.g.}$\\ 
$\text{Target Generation}  \leftarrow \text{Gen.}$ \,\,\,\,\,\,\,\,  \\ 
$\{s, a, r\} \leftarrow \text{Environment Learning: ExploringStarts (e.g.)}$ \\
$(\pi_{A}, \pi_{C}) \leftarrow \text{Navigation Learning: PPO (e.g.)}$ \\
$\text{Gen.} \leftarrow \text{Generation}(\pi_{A}, \pi_{C}, [\text{CrossDL}]_{\mathcal{D}_{f}}).  \,\,\,\,\,\,\,\square$
\end{algorithmic}
\end{algorithm}



\noindent \textbf{One elaboration on CNN-encoding of vision observations.}
Consider the testing for a learner by reinforcement. 
State start $s_{0}$: observation $\mathcal{D}$. Proceed, e.g., as follows.
\begin{align*}
\mathcal{D} &\overset{\text{obs-CNN}}{\mapsto}
(\mathcal{D}_{1}, \mathcal{D}_{2}, \dots, \mathcal_{D}_{\text{k}}) \\
&\overset{\text{obs.-CNN}}{\mapsto}
(\mathcal{D}_{11},
\dots,
\mathcal{D}_{1k},
\dots,
\mathcal{D}_{k1},
\dots,
\mathcal{D}_{kk}
) \\
& \dots \\
&\overset{\text{obs.-CNN}}{\mapsto}
\text{"CombPer"}(
(\mathcal{D}_{i_{1}, \dots, i_{n}})_{i_{1},\dots,i_{n} \in \{1,\dots n\}   })
\\
&\overset{L}{\mapsto}
\text{"CombPer"}((L_{i_{1}, \dots, i_{n}})_{i_{1},\dots,i_{n} \in \{1,\dots n\} }).
\end{align*}


Note that $\mathcal{D}$ is indexable to an order that is exponential to the amount of applications of \text{obs-CNN} to observation $\mathcal{D}$. For a representation of states for, e.g. [NavigationDRL], denote therefore
\begin{align*}
s_{0} &= ( \mathcal{D}_{m_{i}})_{i=1}^{r}\\
s_{1} &= ( \mathcal{D}_{l_{i}})_{i=1}^{r}\\
s_{2} &= ( \mathcal{D}_{k_{i}})_{i=1}^{r}\\
& \dots \\
\,\,\,\,\,\hat{r}_{i} &= L_{s_{i}} - L_{s_{i-1}}, \,\,\,\,\,i \geq 1\\
\,\,\,\,\,\tilde{r}_{i} &= -\hat{r}_{i}, \,\,\,\,\,i \geq 1  \\
\,\,\,\,\, {r}_{i} &= \{\hat{r}_{i}, \tilde{r}_{i}\}_{i \geq 1}, \,\,\,\,\,i \geq 1 \,\,\,\,\,\,\text{Simplest loss-configuration}.
\end{align*}
For positive $r$, seek optimal loss-configuration, via, e.g. an agent-configuror (see deployment 1.3 for an example hereof). Actions bring transition between states, practical examples of which can be found in drone-rotation, or movement of a virtual entity such as BipedalWalker-v2). Note that, in this simple setting, actions only allow transition between states that are distanced in index by 1. Transition probabilities $p(a|s)$ defined by noise and imperfection in observer and environment may be approximated by initial environment exploration. Policy $\pi_{\theta} (a|s)$ to be learned by, e.g., DQN. At terminal, settled state, reconsider all kernels (circularities allowed and encouraged).
Indication for completion exploration phase may lie in a detected circularity in random traversal. The above example serves to illustrate, and is indeed most simple.

The storage of data by vision is highly general; consider the encoding of sound in audiospectograms, and the encoding of temperature via InfraRed lenses. The extent of preconceptualization in vision is measured by the scope of multi-channel CNN-application, therefore continue the procedure of data-point-entanglement until pre-defined threshold reached. Vision-adequacy. Assurance of threshold crossing by movement over environment of deployment can be granted by movement over environment, to give encodings for state-space $\mathcal{S}$ by and for [WalkerDRL]. Feature-collection with provided trace, a special case of vision-adequacy. Feature-collection with considered history of feature observation for capture of feature-movement.

The architecture of the vision CNN-Encoder constructor is a derivative of AlphaChip Policy Network Architecture. One might inquire into the nature of such a constructor, and one constructor of the greatest simplicity may be formulated as follows. An alternative to AlphaChip, of the greatest simplicity: CNN-Encoder construction by greedy feature-identifier selection. Feature-identification by CNN-Encoding with gradual addition of convolutional layers of dimension $C_{i} \times C_{o} \times K \times K$, where $K$ defines the kernel size for specificity in feature identification, $C_{i}$ is the input-channel size (depends on vision capturer via depth/color-encoding etc.) and $C_{o}$ is defined by the out-channel size (sets the count of features to be considered in loss-determination and environment generation).




Up-/Down-sampling by (un)pooling capturable through specification $K$ and $C_{o}$. Selection by input-type of $K, C_{i}, C_{o}$. One selection of an algorithmic nature reads as follows. For subsequent layers, reiterate the above algorithm with input-dimensions equal to the 
back-layer of the convolutional neural network to be expanded. One might define another overarching algorithm for a search of the optimal amount of layers with corresponding optimal layer-dimensions and network-propagation properties. A brute-force approach such as taken in the above algorithm can however come with excessive computational cost.
One other approach lies in a setting of constructing the Conv2D network by Chip Placement by Deep Reinforcement Learning, see AlphaChip. \\

\noindent \textbf{Readily Constructible Absolute Minimum One.} The following does skip over many details. \textit{Design of absolute minimality}. \textit{[\text{CrossDL}]} contains only DNN, \newline
[\textit{WalkerDRL}] defined by 
$\mathcal{S}$, pixel-value-set of vision-input of part of environment,
$\mathcal{A}$, device movement over $\mathcal{S}$,
$\mathcal{R}$, Loss-configuration over losses,
$p(a|s)$: device-dependent, to be computed by explorative simulation (e.g. by MC) over environment.
On-policy learned $\pi(a|s)$: traversal route over $\mathcal{S}$. Target specification: long measurement by Vision prolonged observation by measuring device in portable oculary.
Time-interval between movements tweakable, e.g. 4 seconds to 4 days depending on activity-intensity over $\mathcal{S}$. Local optimum if any action results in negative reward, after which settle for optimal optimum.

Complication in transitions $p(a|s)$. State-space $\mathcal{S}$ may also be defined by location, or orientation, whence only rewards are derived from Vision observation. Stochasticity in $p(a|s)$ stems from device imperfection or action stochasticity, although determinism in $p(a|s)$ is no obstacle.

\textit{SoftwareReady.} Improvements are possible in definition $s$ in $\mathcal{S}$, in feature-consideration for $r$ in $\mathcal{R}$, in device of observation and deployment (Drone better than non-mobile device) and $a$ in \mathcal{A}, in target representation, $[\text{crossDL}]$ configuror component addition (configurors for optimization over improvement dimensions) (RL-optimization). For an extension, consider state-space mixtures in $\mathcal{S}$. \\

\noindent \textbf{First Data-space Set-up.} Deployment of algorithms are dependent on synchronization in index, by synchronization in time-indices and by synchronization in location-indices, a combination of indices of varying natures that have been synchronized, or new specification. Time-delay poses no obstacle to synchronization over indices that include a time-index. The synchronization exists between input and output realizations, and histories may be used since chronology bears more importance to functionality.

For deployment by simulated oculary in virtual space, data-capture is near automatic. For capture of data for dataspaces that might be more esotoric, first define indices over which to reset synchronization, and proceed with the above algorithms. In simulated space, all data-structure is enforcable by built-in programming. In spaces of data that have been constructed, dataspace distance is first defined, and for synchronization over indices other than time does one need a sense of order over the indices more than enumeration.





Evolution in output due to evolution in corresponding realizations can occur in various ways. Movement of the evolution can occur by an oculary's movement, or landscape's movement, or the activity of an EEG. (It is) It is the evolution in its entirety that must be defined. The same holds for the input, although an agent's movement itself captures part of the evolution (the agent's necessity follows the unobservability of one dataspace in its completion. Hence, a similar agent might be deployed over output dataspace.)


Conceptualization in its many forms would gain improvement upon completed simulation and obtained elements of defect. The RL-lens-sharpener-route of conceptualization is one framework through which to implement conceptualization and tweaking its specificity may give improvement, or a translation of another implementation. The RL-route
gives leeway for continuous or discrete, (pre-learned) iterative sharpening, an attribute that is not shared with manual initialization or basic grid-search automated within time-intervals.  

In the current implement, the lens sharpness is adjusted by a learner that has learned or is learning the optimal adjustment of lenses. The observer's navigation protocol
has been pre-learned or is being learned by the exploring observing. This is the first implementation, most general. \\

\noindent \textbf{Exhibitability One.} An observer's interpreter drives the program. 
Thus seek for many interpreters. Envision interpreter for every setting
over which knowledge retrieval might be desired, or needed for furtherment.
Data can take many forms, thus would a setting-encoding network be better than interpretation after encoding? What other ways of traveling over a setting exist? A reinforcement learner has in its core built-in the policy, equippable by an observer. Would observation over a setting in its completion be necessary in some situations due to a traveler's restriction? How would such an activity occur? Worlds are big indeed.

Lens construction occurs by layer-after-layer assembly of a convolutional neural network according to a trained reinforcement learning agent that steers the type and specification and depth of each layer in the network. And this has shown effective in the chip-placement analogy. A similar demonstration would have to show the same effectivity for lens construction. What other ways of observing might there exist? Surely might every network be constructed according to a learner's dictation.   


How else might an optimal lens be chosen? To be equipped with many alternations beforehand gives clearance over the flow through which to seek for solutions that truly (work). For code swiftly follows design.



The nature of observation and linking lies at the heart of the programs and their designs, and there are many technologies existent for deducing the one from the other. Sending rockets to mars is one way to become familiar with the planets. To deploy submarines is one way to become familiar with planetary depths. Another way is to seek for theory and to observe the plain a bit more sharply, and thereby deduce all the rest. Everything connected with everything is an idea not void, and so does the matter of asking the right questions, and answering every question that one might bring to rise be a mere matter of seeking the right notions and deducing thereafter. This is the idea of preconceptualization. An observer is an interpreter, i.e. conceptualizer, and subsequent preconceptualizer as one completion. For conceptualization does there exist many implementations. \\

\noindent \textbf{Compatibility One.} For compatibility, investigate conventional settings in the current framework. For exhibitability, apply the current framework to settings that extend conventional settings. Comptabitibility setting examples  are to be found on canvas, usual book examples on any subject. Exhibitability setting example: visualize in graphs and deploy virtual agent over graph for any sought after deduction. 

These written works dictate the direction, code swiftly follows. \\(Mathematical software and software follow, software without design is mere computation without aim, mathematics is implied in all written works (see references for details), software is implied in all written works (see references for details),
the design that the written works contain lie at the core, any convoluted language bears mountain beneath visible tip, there are constraints to which one must adhere.
The written works dictate the direction, code swiftly follows.) \\

\noindent \textbf{Hypersymplectic illustration One.}
Illustration of generalization by considering hypersimplices. Let ordinary state-space be denoted by $\mathcal{S}$. Consider $s' \in \sigma(\mathcal{S})$ and take scaled hypersimplex over $s'$ for $s*$ in new state-space $\mathcal{S}'$. Hypersimplex denotes duratio of device in each ordinary state in collection of states (discretization for realizability). Definition action-space by direction of shrinkage, direction of expansion, fluctuation of hypersimplex (restricted to those with sum bounded by constant L), length of expansion. Definition loss by error in generation. Definition $r$ by reward configuration over losses between states and corresponding action (one simple example is Loss reduction) \\

\noindent \textbf{Example. First (and quick) technicality check by solving mathematical problems.} The following is preliminary and serves to illustrate. There is more to come, deductions more 'advanced'.

\noindent \textbf{Sample.} \textit{Let location equal acceleration. Express location in terms of time. This problem can be expressed in Differential Equations. The below is one simple illustration, and more complexity might follow similar lines. Clear example for utility in solution is efficiency gains for software-encoding of movement reinforcement-learning agent.} 

\textbf{One.}
We have that $x(t) = e^{tA}x_{0}$ with A diagonalizable, hence \\ $x(t) = C\,\text{diag}(e^{\lambda t})\, C^{-1}x_{0} = \sum_{j=1}^{m} v_{j} \text{exp}(\lambda_{j}t) c_{j}$ with $c_{j} = C^{-1}x_{0}e_{j}$ and $v_{j} = Ce_{j}.$ 

\textbf{Two.} Given $\partial^{2}y(t)/\partial dt^{2} - y(t) = 0$ and $x(t) = (y(t), \partial y(t)/\partial t)$. Then $\partial^{2}y(t)/\partial t^{2} - y(t) = \partial^{2}y(t)/\partial t^{2} + \partial y(t)/\partial t - \partial y(t)/\partial t - y(t) =: A'\partial x(t)/\partial t  + B' x(t) = 0$. Extend matrices $A'$ and $B'$ to invertible matrices to obtain extensions $A$ and $B$ respectively for $\partial x/\partial t = A^{-1} B x(t) = C x(t)$ for $C:=A^{-1}B$. Solution is given by $x(t) = e^{tC}x(0)$. Illustratory extensions $A' \mapsto A$ and $B' \mapsto B$ are given by $A=\left[[1 \,\,0]^{\top}\,\,[-1\,\, 1]^{\top}\right]$ and $B=\left[[-1 \,\,0]^{\top}\,\,[1\,\, 1]^{\top}\right]$. 

 \textbf{Three.}
Let $A=\left[[\lambda \,\,1]^{\top}\,\,[0\,\, \lambda]^{\top}\right]$. Then $A^{n} = $$A^{n}=\left[[\lambda^{n} \,\,(n-1)\lambda^{n-1}]^{\top}\,\,[0\,\, \lambda^{n}]^{\top}\right]$. Induction: 
$A^{n}A=\left[[\lambda^{n} \,\,(n-1)\lambda^{n-1}]^{\top}\,\,[0\,\, \lambda^{n}]^{\top}\right] \left[[\lambda \,\,1]^{\top}\,\,[0\,\, \lambda]^{\top}\right] = \\
\left[[\lambda^{n+1} \,\,n\lambda^{n}]^{\top}\,\,[0\,\, \lambda^{n+1}]^{\top}\right] =: A^{n+1},$ indeed. 
    
 \textbf{Four.} Solution for $\partial x(t)/\partial t = Ax(t)$ with $x(0):=x_{0}$ is given by $x(t) = e^{tA}x_{0}$. Note that $A =
    \left[[1 \,\,1]^{\top}\,\,[1\,\, 0]^{\top}\right] \overset{A}{\mapsto}
    \left[[2 \,\,1]^{\top}\,\,[1\,\, 1]^{\top}\right] \overset{A}{\mapsto}
    \left[[3 \,\,2]^{\top}\,\,[2\,\, 1]^{\top}\right] \overset{A}{\mapsto}
    \left[[5 \,\,2]^{\top}\,\,[3\,\, 2]^{\top}\right] \\ \overset{A}{\mapsto}
    \left[[8 \,\,5]^{\top}\,\,[5\,\, 3]^{\top}\right] \overset{A}{\mapsto}
    \dots
    \overset{A}{\mapsto} \left[[34 \,\,21]^{\top}\,\,[21\,\,  13]^{\top}\right] \overset{A}{\mapsto}
    \dots
    \overset{A}{\mapsto}
    \left[[F_{n} \,\,F_{n-1}]^{\top}\,\,[F_{n-1}\,\, F_{n-2}]^{\top}\right].
    $
    For general form $A^{k}$ use diagonalizability of $A$. Denote the 'Golden Ratio', as known from Fibonacci Sequence, by $\phi$. Eigenvectors $v_{1}, v_{2}$ of $A$ are $v_{1} = [+\phi\,\,\,1]^{\top}$, $v_{1} = [-\phi\,\,\,1]^{\top}$ with respective eigenvalues $\lambda_{1} = \phi$ and $\lambda_{2} = \phi$. Next solve $x(t) = c_{1} e^{\phi t} [\phi \,\,\, 1]^{\top} + c_{2} e^{-\phi t} [-\phi\,\,\, 1]^{\top}$ for $c_{1}, c_{2}$. Fix $t=0$ to obtain the system of equations $[1\,\,\,\, 0]^{\top} = [1 \,\,\, 0]^{\top} = c_{1} e^{\phi t} [\phi \,\,\,1]^{\top}+  c_{2} e^{-\phi t} [-\phi \,\,\,1]^{\top}].$ Expression for $c_{1},c_{2}, x(t)$ is given by
    $c_{2} = (e^{-\phi t} \phi - e^{-2\phi t} \phi)^{-1}$,
    $c_{1} = -(e^{-\phi t} \phi - e^{-2 \phi t} \phi)^{-1} e^{-2\phi t}$,
$x(t) = -(e^{-\phi t} \phi - e^{-2 \phi t} \phi)^{-1} e^{-\phi t} [\phi \,\,\, 1]^{\top} + (e^{-\phi t} \phi - e^{-2\phi t} \phi)^{-1} e^{-\phi t}
[-\phi\,\,\, 1]^{\top}$. \\

\noindent \textbf{Matrix-exponentials sample computations.} Consider 
\begin{equation*}
A^{k} =
\begin{bmatrix}
\lambda^{k} & (k-1)\lambda^{k-1} \\
0 & \lambda^{k}
\end{bmatrix}.
\end{equation*}
Next define,
\begin{equation*}
N    
= 
\begin{bmatrix}
0&  1  & 0 & \dots&  0  & 0\\
0 & 0 & 1&  \dots  & 0 & 0\\
\vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0 &  0  & 0&  \dots  & 1 & 0\\
0&  0  & 0&  \dots  & 0 & 1\\
0 & 0 & 0&  \dots  & 0 & 0\\
\end{bmatrix}.
\end{equation*}
Assume compatibility of matrix dimensions in all computation. 
\begin{align*}
\tilde{A} &= \lambda I + N \\
&= 
\begin{bmatrix}
\lambda&  0  & \dots&  0  & 0\\
0 & \lambda &   \dots  & 0 & 0\\
\vdots & \vdots & \ddots  & \vdots & \vdots \\
0&  0  &   \dots  & \lambda & 0\\
0 & 0 &   \dots  & 0 & \lambda\\
\end{bmatrix} 
+  
\begin{bmatrix}
0&  1  & 0 & \dots&  0  & 0\\
0 & 0 & 1&  \dots  & 0 & 0\\
\vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0 &  0  & 0&  \dots  & 1 & 0\\
0&  0  & 0&  \dots  & 0 & 1\\
0 & 0 & 0&  \dots  & 0 & 0\\
\end{bmatrix} \\
&= 
\begin{bmatrix}
A &  0  & \dots&  0  & 0\\
0 & A &   \dots  & 0 & 0\\
\vdots & \vdots & \ddots  & \vdots & \vdots \\
0&  0  &   \dots  & A & 0\\
0 & 0 &   \dots  & 0 & A\\
\end{bmatrix} 
+  
\begin{bmatrix}
0&  1  & 0 & \dots&  0  & 0\\
0 & 0 & 1&  \dots  & 0 & 0\\
\vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0 &  0  & 0&  \dots  & 1 & 0\\
0&  0  & 0&  \dots  & 0 & 1\\
0 & 0 & 0&  \dots  & 0 & 0\\
\end{bmatrix} \\
&\overset{\tilde{A}}{\mapsto}
\begin{bmatrix}
A^{3} &  0  & \dots&  0  & 0\\
0 & A^{3} &   \dots  & 0 & 0\\
\vdots & \vdots & \ddots  & \vdots & \vdots \\
0&  0  &   \dots  & A^{3} & 0\\
0 & 0 &   \dots  & 0 & A^{3}\\
\end{bmatrix} 
+  
\begin{bmatrix}
0&  \lambda^{2}  & 0 & \dots&  0  & 0\\
0 & 0 & \lambda^{2}&  \dots  & 0 & 0\\
\vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0 &  0  & 0&  \dots  & \lambda^{2} & 0\\
0&  0  & 0&  \dots  & 0 & \lambda^{2}\\
0 & 0 & 0&  \dots  & 0 & 0\\
\end{bmatrix} \\
&\overset{\tilde{A}}{\mapsto}
\begin{bmatrix}
A^{4} &  0  & \dots&  0  & 0\\
0 & A^{4} &   \dots  & 0 & 0\\
\vdots & \vdots & \ddots  & \vdots & \vdots \\
0&  0  &   \dots  & A^{4} & 0\\
0 & 0 &   \dots  & 0 & A^{4}\\
\end{bmatrix} 
+  
\begin{bmatrix}
0&  \lambda^{3}  & 0 & \dots&  0  & 0\\
0 & 0 & \lambda^{3}&  \dots  & 0 & 0\\
\vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0 &  0  & 0&  \dots  & \lambda^{3} & 0\\
0&  0  & 0&  \dots  & 0 & \lambda^{3}\\
0 & 0 & 0&  \dots  & 0 & 0\\
\end{bmatrix} \\
&\overset{\tilde{A}}{\mapsto} \dots \overset{\tilde{A}}{\mapsto}  
\begin{bmatrix}
A^{n} &  0  & \dots&  0  & 0\\
0 & A^{n} &   \dots  & 0 & 0\\
\vdots & \vdots & \ddots  & \vdots & \vdots \\
0&  0  &   \dots  & A^{n} & 0\\
0 & 0 &   \dots  & 0 & A^{n}\\
\end{bmatrix}
+  
\begin{bmatrix}
0&  \lambda^{n-1}  & 0 & \dots&  0  & 0\\
0 & 0 & \lambda^{n-1}&  \dots  & 0 & 0\\
\vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0 &  0  & 0&  \dots  & \lambda^{n-1} & 0\\
0&  0  & 0&  \dots  & 0 & \lambda^{n-1}\\
0 & 0 & 0&  \dots  & 0 & 0\\
\end{bmatrix}.
\end{align*}
Recall matrix exponential $e^{tA}$ given by
\begin{equation*}
e^{tA} =
\begin{bmatrix}
e^{\lambda} & \left(\frac{\lambda - 1}{\lambda} \right)  e^{\lambda} \\
0 & e^{\lambda} 
\end{bmatrix}. 
\end{equation*

Matrix exponential $e^{t\tilde{A}}$ is given by
\begin{equation*}
e^{t\tilde{A}} = 
\begin{bmatrix}
e^{\lambda} &  0  & \dots&  0  & 0\\
0 & e^{\lambda} &   \dots  & 0 & 0\\
\vdots & \vdots & \ddots  & \vdots & \vdots \\
0&  0  &   \dots  & e^{\lambda} & 0\\
0 & 0 &   \dots  & 0 & e^{\lambda}\\
\end{bmatrix}
+  
\begin{bmatrix}
0&  \left(\frac{\lambda - 1}{\lambda} \right)  e^{\lambda}   & 0 & \dots&  0  & 0\\
0 & 0 &\left(\frac{\lambda - 1}{\lambda} \right)  e^{\lambda} &  \dots  & 0 & 0\\
\vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0 &  0  & 0&  \dots  & \left(\frac{\lambda - 1}{\lambda} \right)  e^{\lambda} & 0\\
0&  0  & 0&  \dots  & 0 & \left(\frac{\lambda - 1}{\lambda} \right)  e^{\lambda} \\
0 & 0 & 0&  \dots  & 0 & 0\\
\end{bmatrix}.
\end{equation*}


\noindent \textbf{First Practical Detail.} Mathematics, SoftwareEngineering, ComputerScience. Simulation Engines: Game Engines: Unity, UnrealEngine, UbiSoftFramework, Anvil. Engines for simulation with pre-installed setting: Celestia, Unity, NVIDIA PhysX, Ansys Fluent, Advanced Simulation Library (OpenSource). Computers: Cloud: LambdaLabs, etc. Scripts deployable in open-world games or engines of simulatio, mainly for capture of virtual setting interconstruction: Heaviest PC Game-type: RTX-9040 Orion Predator Acer, Vector GPU Desktop/Workstation. Gadgets: IoT STM3224 Discovery kit node, ESP32-Azure IoT Kit, IoT-aided artifacts, oculary-autorotators, engine simulation before robotics, oculary must be very high definition.

Mathematics, SoftwareEngineering, ComputerScience. Simulation Engines: Game Engines: Unity, UnrealEngine, UbiSoftFramework, Anvil. Engines for simulation with pre-installed setting: Celestia, Unity, NVIDIA PhysX, Ansys Fluent, Advanced Simulation Library (OpenSource). Computers: Cloud: LambdaLabs, etc. Scripts deployable in open-world games or engines of simulatio, mainly for capture of virtual setting interconstruction: Heaviest PC Game-type: RTX-9040 Orion Predator Acer, Vector GPU Desktop/Workstation. Gadgets: IoT STM3224 Discovery kit node, ESP32-Azure IoT Kit, IoT-aided artifacts, oculary-autorotators, engine simulation before robotics, oculary must be very sharp. \\

\noindent \textbf{First Architectural Design Detail: First Architecture 1: Legend over hyperparameters.} 
[Architectural Design Detail: Architecture 1: Legend over hyperparameters.] First expression of project in terms of popular notions.
\begin{itemize}

    \item \textbf{[CrossDL] = [CrossCNNRNN]}
    
    \item \textbf{[WalkerDRL]} requires care in definition space $S$, framework POMDP possible. 

    \item Test architectures by deployment, i.e. grade constructions by simulation 

    \item ($\mathcal{D}$-dependent) obs. $\text{CNN}_{\mathcal{D}_{f}}$ (single convolutional layer)

    \item \textbf{n} (Iteration count application convolutional layers)

    \item \textbf{r'} (Consideration trade-off training multiple small networks  versus training one big network)

    \item \textbf{$\lambda_{n}$} (Consideration trade-off training multiple small networks  versus training one big network)

    \item \textbf{$s$} (Under optionality of hyperparameter $r'$, subscript $\mathcal{D}$ takes various definitions). In other words, different hyperparameter inclusion for Vision, different state representation. $[CrossDL]_{\mathcal{D}_{f}}^{s}$ might be 1 network, with $\mathcal{D}$-dependent obs. $\text{CNN}_{\mathcal{D}_{f}}$ stacked after the previous, resulting in an optimal network that might bear the intepretation of an optimal lens)

    \item \textbf{$h_{s}$} (definition state history, either static past observations over time intervals, or past sequences of static observation within one time-frame, corresponds to definition $s$)

    \item \textbf{Directions} Learner achieves performance objectives deducible from heuristic visual. New objectives and devices following from old construction of old devices.

\end{itemize}

\noindent \textbf{First Architectural Design Detail: VisionCNN Architecture, Architecture 2: Legend over hyperparameters.} 
[Architectural Design Detail: VisionCNN Architecture, Architecture 2: Legend over hyperparameters.] First expression of project in terms of popular notions.

\begin{itemize}

    \item \textbf{CNNoptimization} (hyperparameter in [$\text{Cross-
    DL}_{\mathcal{D}_{f}}^{s}$] for optimization, or training, optimal CNN for $s$. Examples Defintion Loss measure, hyperparameters in optimization algorithm (velocity, parameters).)
    
    \item \textbf{RNNoptimization} (hyperparameters in [$\text{Cross-
    DL}_{\mathcal{D}_{f}}^{s}$] for optimization optimal CNN-RNN for $s$. Examples Definition Loss-measure, hyperparameters in optimization algorithm. Definition reards, for example by specification of loss-configuration between current state and subsequent state, one example specification is the difference in Loss)

    \item \textbf{a} (type of movement for search preconceptualization, depends on search-space, and device of application. See BipedalWalker-v2, e.g. application by drone similar, or traversal dataspace, settlement LunarLander)

    \item $\mathbf{p(a|s)}$ (deterministic mainly, stochasticity due to uncertainty in environment or device of application)

    \item $\mathbf{\pi_{\theta}(a|s)}$ \textbf{optimizable network}. (Hyperparameters in Learning algorithm for optimization $\pi_{\theta}(a|s)$, immediate exploration dictates on-policy learning, for example by Proximal Policy Optimization (PPO))


    \item \textbf{Directions} Learner achieves performance objectives deducible from heuristic visual. New objectives and devices following from old construction of old devices.

\end{itemize}




\noindent \textbf{First Speculative Avenues of Pursuit I.} Speculative Avenues of Pursuit I. First expression of project in terms of popular notions.
\begin{itemize}
    \item 
    Multiple observers, multi-agent RL
    \item Observer sophistication and device complexity
    \item Multi-agent, multi-observer swarm deployment
    \item Output-type numeric encoding for alignment with convention
    \item Continuous observer interpreter readjustment
    \item Deployment in settings of sure expected functionality
    \item Generalization to new settings to be ancored in deployment to setting of sure expected functionality
    \item Input sensor interpreter other than convolutional neural network
    \item Intantaneous setting transformation upon retrieval
    \item Multi-agent, multi-observer for both input and output with static central control coordinator
    \item Every technological vehicle, under specialization to its setting can function as an observer, either autonomous or not.    
\end{itemize}

\noindent \textbf{First Speculative Avenues of Pursuit II, One: Network Design Initialization.} First expression of project in terms of popular notions.
\begin{itemize}
    \item 
    Multiple observers, multi-agent RL over setting of deployment and simulated mirages for amplification of subsets of setting features
    \item GlobeMap, MolStruct, Computational chip can all serve as settings, or layers in simulated mirage of greater setting from where these can be extended
    \item Results in deep generative network will serve as a basis for observing beyond the plain
    \item Observation encoding/decoding is not necessary for functionality, but is necessary for generation correctness
    \item In every multi-agent deployment can a central coordinator steer over the above points
    \item Simple settings will serve as foundations for complex setting for correctness verification
    \item Human interface through, e.g. linguistic expression of output
\end{itemize}

\noindent \textbf{First Readily Constructible Avenues of Pursuit One.} Readily Constructible Avenues of Pursuit. First expression of project in terms of popular notions.
\begin{itemize}
    \item Learners over virtual space, including IoT Networks
    \item All the notes written up to the present
    \item Evaluation performance implicit in (D)RL and DL losses. Heuristic visual serve as heuristic guidance.
    \item Specification of objectives
    \item Device programming and assembly ready for deployment 
    \item Monitoring Learner activity and performance
    \item Deployment in virtual setting Unity PhysX on small computer
    \item Deployment in open-world games on big computer
    \item Deployment over IoT with or without external gadget
    \item Multi-agent extension towards first most general implementation
    \item Deployment on every usual task in data analysis
    \item Continuous lens-re-adjustment over visor movements
    \item Addition or subtraction of complexity in Learner's construction, more powerful device can give and run more complex learner in the same framework current, whether this is good is to be observed from performance monitoring
    \item Gain in sum of existing trade-offs by lowering movement capacity for higher observation capacity
    \item Lens construction is setting-dependent and setting-distances are readily defined, or learned
    \item Written documentation for technical specification in lens construction by sequential placement of elements
    \item User-defined travel for observation beyond the plain
    \item Acquire necessary devices    
\end{itemize}
\noindent \textbf{First Stages of development.}
First full practical implementation in starting simulation software can occrue according to the steps below. Unity is one starting engine, and the below outlines implementation concretualization.
\begin{enumerate}
    \item \textbf{Script Writing.} Any notebook would suffice for Script Writing. We settle for the most rudimentary digital notebook that \textbf{Apple} has to offer.
    \item Manually crafted virtual space deployed machine in assembly CNN-RNN-DMARL or CNN-RNN-DRL, using any text-editor or code-editor
    \item (Engine) prepared virtual space deployed machine in assembly CNN-RNN-DRL with oculary-agent in assembly CNN-RNN-DRL with (Engine) oculary-agent role taken by built-in ocularys by installing code-editor agent with (Egine) launcher. Evolve from simple environments to complex environments, e.g. $2D \rightarrow 3D$. Account for the possibility of speculative improvements, implementable, e.g., in game engine.
    \item Explore different Egines, beyond MuJoCo's Unity-plug-in or the PhysX-Unity subEngine for deployment virtual open-world simulation of complex and detailed environments for great information capture, and much space for potential low loss processing, i.e. potential comovement. Improve machines if improvement found by abnormality in Engine capacity or monitored results or new inspiration.
    \item Explore machine-deployment in the real-world, programmed according to machine-deployment in the virtual-world.
\end{enumerate}

\noindent \textbf{Star-systematic configuration for illustration of direction for extension in Deep Learning.}
Having access to all that surely knows currently, infer the rest as good as possible. Features, settings, assumed. $\text{Microscale} \rightarrow \text{Exascale}$. Datatypes: Numeric Array, Image, Sound (convertible to Image), Numeric Array $\rightarrow$ Image by Image Generator, DeepDream. Build then extend, learning links of links, learning links, notions: order of links; links between environments. The links in every intralink-vertex are the datasets, and every intralink-vertex in generated by their respective aspect of deployment in Data-space. Start rudimentarily with 5 nodes. Implement and report performance. Example. Inquiry towards $D5$, access to $D1$ and $D2$: try $D1 \rightarrow D5$, $D2 \rightarrow D5$, try $D1 \rightarrow D2 \rightarrow D5$ and $D2 \rightarrow D1 \rightarrow D5$. The complete bidirectional graph in Figure 1 is labeled by $(D_{1},D_{2},\dots, D_{5})$, and each of the two edges between any two distinct nodes is connected by edges in both directions hither and thither respectively.

\begin{figure}
\centering
\begin{tikzpicture}[bend angle=2]
\graph[
  new <->/.code n args={4}{
    \path[->, every new <->/.try](#1\tikzgraphleftanchor) edge[bend left,#3] #4 (#2\tikzgraphrightanchor);
    \path[->, every new <->/.try](#2\tikzgraphleftanchor) edge[bend left,#3] #4 (#1\tikzgraphrightanchor);
  }
]{
  subgraph K_n [
    V={D1, D2, D3, D4, D5},
    clockwise,
    radius=2cm,
    nodes={draw, circle},
    <->
  ]
};
\end{tikzpicture}
\caption{Complete bidirectional graph for a so-called DL-StarSystem of the fifth order, vertices represent intralinks, edges represent interlinks. One obvious generalization lies in DL-StarSystems of the $n$'th order, representable by a nomplete bidirectional graph with $n$ vertices. See file \texttt{Ytractorfunctional.py} in repository \texttt{DLStarSystem}.}
\end{figure}

Inquiring into $D2$ requires some knowledge on $D2$ to be extended from depoyment in data-space. Call the above, Starlink Order 5. Starlink Orders 1, \dots 5 work. Generalization to higher orders would but require a heavier computer. Inquire into feature detection. Among the datasets are the vertex-intralinks.  The above graph is bidirectional in every edge. See \textcolor{blue}{ https://github.com/Ali-Al-Habsyi/DLStarSystem} for full implementations.  Herefrom follow extensions that one might indeed deem valuable. \textit{The extension DL-StarSystem serves to illustrate approach towards an extension that fits the preceding descriptions. It bears \textbf{some} basis in mathematics, and one might engineer the system's specification of deployment over dataspace towards grand utility of the system, as one would engineer any ordinary neural network. To bring higher mathematics is our first pursuit. Advanced machines in software and hardware follow. The development of these machines are our next pursuit.}\\



\noindent \textbf{In keywords, Directions Forward Include. } \textit{(full stops "." serve as semicolons ";")}
Continuum supremacy (analogous to 'Quantum Supremacy'). Continuum computing. In the plain can new realms be discovered by considering continuity (Ricci Flow). Representation with $\infty$-information density, as $\mathcal{D}_{f}$ has $\infty$-information density. Computation must have throughput-level $\infty$. Simulation Science captures Computer Science and Discrete Mathematics in the conventional sense. Continuum Science captures all non-discrete mathematics and emphasizes the notion of continuity and $\infty$. Consider interface, or portal, between simulation in full continuity, as captured by reality (or universe), or simulation in partial discretization, as captured by simulation science, to transition from one to the other. Biological, neuro-organic, computation ensures capacity to continuum compute, as evolution happens on platform reality, or universe. For practical demonstration apply portal for transition from continuum to dicrete realm. For research in paradigm of continuum computation as benchmarked by conventional paradigms of computing, apply portal for transition from discrete realm to continuum. For true discovery on universe in only stationary work-station, consideration of continuity is essential. Consider exemplary Millennium Prize Problem Poincar Conjecture. Important in 'Important Accomplishments' in mathematics is relative. Many open problems are more straight-forward than they seem. Millennium Prize Problems are worthy of hunting down.

\noindent \textbf{In keywords, Quick Notes.}
\textit{(full stops "." serve as semicolons ";")}
Information density $\mathcal{D}_{f}$ is without bound. "Mathematics of the continuum": Analysis on Manifolds, Interface/Portal Realms Discrete/Continuous, Modern Algebra, Modern learning (toward Continuum learning), Modern Quantum Computing and Discrete Mathematics (towards Continuum Computing and Continuum Observing), (Analysis on Manifolds captures Multivariate Analysis), (Modern Algebra captures Linear Algebra), (Interface Realms Discrete/Continuous captures Calculus and Integration Theory), (Essential to the interface Realms Discrete/Continuous is the notion of continuity, most variants of which are slight variations of the definition: $f$ is continuous at $a$ if $[\forall \epsilon >0: \exists \delta > 0: |x - a| < \delta \implies |f(x) - f(a)|<\epsilon]$). We seek to generalize. How might such a generalization read? In continuum observing do we seek to extract data $\mathcal{D}_{f}$, infinitely progressive in level of detail from where data-extraction occurs. Great addition to the level of complexity in existing notion of continuity hint towards useful notions for '$\mathcal{D}_{f}$ as a continuum'. Introductory Mathematics of the Continuum. Complex Analysis: formulation traversal Mandelbrot set, Analysis on Manifolds: Greene's Theorem. Algebra of differentials $d$. Assume first observal with infinite capacity and deduce therefrom. (Oculary) Restrictive parametrics in e.g., Extent of scope, Definition of Data-capture, (point-wise, or, e.g., Manifold-encoding) (Statistics over Machinal Configurations) Representations of $\mathcal{D}_{f}$, Information throughput. Continuum Computing, Continuum Learning, Assume perfect machine machine, the ideal scenario. All capacity for information storage and implementation of restrictive parameters. Required are algorithms of the continuum. One route of construction Mathematical Machine. Define Mathematical Machine in mathematical terms, and deduce all further. A practically implementable version of mathematical machine is a mere variant of mathematical machine. Supposition. Introductory Continuum Computing. Pre-collection of mathematical machines over the continuum, just as quantum computing is theory of "Quantum Algorithms". Introductory Continuum Learning. Big milepoint is in formulation of Algorithms of the continuum. This gives Mathematical Ocular, by assembly of continuum observer, continuum computer. Research. Algorithms of the continuum, Extraction of data from the continuum, Formulation Mathematical Machine, Continuum Learning: Learning over the continuum, The statistics of the continuum.
The job is now to seek the right notions. E.g., Line $\rightarrow$ Manifolds, Stochastic processes $\rightarrow$ Manifold Stochastic Fluctuation processes for initialization modeling $\mathcal{D}_{f}$. \\

\noindent \textbf{In keywords, Mathematical Analysis.}
\textit{(full stops "." serve as semicolons ";")}
Complex Analysis: formulation traversal Mandelbrot set, Analysis on Manifolds: Greene's Theorem. Algebra of differentials $d$. Assume first observal with infinite capacity and deduce therefrom. (Oculary) Restrictive parametrics in e.g., Extent of scope, Definition of Data-capture, (point-wise, or, e.g., Manifold-encoding) (Statistics over Machinal Configurations) Representations of $\mathcal{D}_{f}$, Information throughput. 

PDE's, Analysis on Manifolds, Interface/Portal Realms Discrete/Continuous, Modern Algebra, Modern learning (toward Continuum learning), Modern Quantum Computing and Discrete Mathematics (towards Continuum Computing and Continuum Observing), (Analysis on Manifolds captures Multivariate Analysis), (Modern Algebra captures Linear Algebra), (Interface Realms Discrete/Continuous captures Calculus and Integration Theory), (Essential to the interface Realms Discrete/Continuous is the notion of continuity)

Research. Algorithms of the continuum, Extraction of data from the continuum, Formulation Mathematical Machine, Continuum Learning: Learning over the continuum, The statistics of the continuum.
The job is now to seek the right notions. E.g., Line $\rightarrow$ Manifolds, Stochastic processes $\rightarrow$ Manifold Stochastic Fluctuation processes for initialization modeling $\mathcal{D}_{f}$

Relevance towards the above interoperable.
Contribution magnitude measurable by inclusion problem as Millenium Prize Problem. Modelling, Analysis on manifolds.
Stochasticity: Stochastic Processes and Stochastic Analysis. Solve open problems that are slightly related to the above. Analysis on manifolds,
Ricci flow with surgery on three-manifolds, the entropy formula for the Ricci flow and its geometric applications. Stochasticity: Stochastic Processes and Stochastic Analysis.

In consequence, formulate statements that relate to $\mathcal{D}_{f}$ as a continuum. Soon shall the notion of 'continuum' be made precise.
Follow pattern exemplary in development QFT.
On the way, solve problem YangMills existence and mass gap.
Analysis on manifolds, Stochasticity: stochastic processes and stochastic analysis. Solve open problems that are slightly related to the above.
ClayMathInstitute. Either prove or disprove.

Analysis on manifolds. Ricci flow with surgery on three-manifolds, The entropy formula for the Ricci flow and its geometric applications $\rightarrow$ Poincar conjecture
stochasticity: stochastic processes and stochastic analysis,
Complex analysis, Riemann Hypothesis.
This, in fact, is not at all difficult.\\

\noindent 
\textbf{Essay Millenium Problems towards solutions Milenium Problems towards solution of this Essay, this File.} \textit{This document contains the essay on the final problem to be solved.}
\begin{itemize}
\item Differential Equations $\rightarrow$ Navier-Stokes Equation, 
\item Complex analysis $\rightarrow$ Riemann Hypothesis, 
\item Quantum Field Theory $\rightarrow$ YangMills existence and mass gap
(Development QFT (Quantum Field Theory) $\iff$ Development '$\mathcal{D}_{f}$ as a continuum'), 
\item Discrete Mathematics: Computing and Algorithms $\rightarrow$ P versus NP, 
\item Algebra's of differentials (see Analysis on manifolds) $\rightarrow$ Hodge conjecture, Birch and Swinnerton-Dyer conjecture, 
\item Analysis on manifolds $\rightarrow$ Specification definition '$\mathcal{D}_{f}$ as a continuum'. 
\item Stochastics $\rightarrow$ Specification definition 'Statistics of the continuum $\mathcal{D}_{f}$'.
\end{itemize}

 
\noindent \textbf{In other words.} Sub-problems that serve to solve the final problem.
\begin{enumerate}
\item Navier-Stokes Equation
\item Riemann Hypothesis
\item YangMills existence and mass gap
\item P versus NP
\item Hodge conjecture
\item Birch and Swinnerton-Dyer conjecture
\item Collatz Conjecture
\item Specification definition '$\mathcal{D}_{f}$ as a continuum'
\end{enumerate}


\noindent \textit{This document contains the essay on the final problem to be solved.}

\noindent\makebox[\linewidth]{\rule{\paperwidth}{2pt}}\\

\footnotesize
\noindent \textbf{Perelman.} \textit{Solver of the Poincar Conjecture.}  \\@misc{perelman2003ricciflowsurgerythreemanifolds,
      title={Ricci flow with surgery on three-manifolds}, 
      author={Grisha Perelman},
      year={2003},
      eprint={math/0303109},
      archivePrefix={arXiv},
      primaryClass={math.DG}, \\
      url= \\{https://arxiv.org/abs/math/0303109}, 
} \\
@misc{perelman2002entropyformularicciflow,
      title={The entropy formula for the Ricci flow and its geometric applications}, 
      author={Grisha Perelman},
      year={2002},
      eprint={math/0211159},
      archivePrefix={arXiv},
      primaryClass={math.DG}, \\
      url={https://arxiv.org/abs/math/0211159}, 
} \\
@misc{Mochizuki2020kyotointeruniversal,
      title={INTER-UNIVERSAL TEICHMULLER THEORY I: CONSTRUCTION OF HODGE THEATERS}, 
      author={Shinichi Mochizuki},
      year={2020}, \\
      eprint={math/0211159},
      archivePrefix={arXiv},
      primaryClass={math.DG}, \\
      url={https://www.kurims.kyoto-u.ac.jp/~motizuki/Inter-universal%20Teichmuller%20Theory%20I.pdf}, 
} \textit{Consider exemplary Millennium Prize Problem Poincar Conjecture. Important in 'Important Accomplishments' in mathematics is relative. Many open problems are more straight-forward than they seem.}


%\printbibliography
% https://www.youtube.com/watch?v=8IMkTnR_KcA&ab_channel=ShareLaTeX

% Attempt at debug
% \bibliographystyle{plain} % We choose the "plain" reference style

%\nocite{*}   % all not cited bib entrys are shown in bibliography ...
%\bibliographystyle{plain} % <===========================================
%\bibliography{example} % to use file created by filecontents ...

% Attempt at debug
% Attempt at debug
% Attempt at debug
% TASK: FIX BIBILIOGRAPHY. BIBLIOGRAPHY CREATION COMPLETION 94%



\noindent \textbf{More directions.} 
\textit{Hyperlinks that may prove useful.} \\
(https://www.claymath.org/millennium-problems/) \\
https://www.marl-book.com/download/marl-book.pdf \\
https://www.tudelft.nl/en/2024/eemcs/cutting-edge-multi-oculary-system-for-multi-modal-data-capture \\
https://arxiv.org/pdf/2312.11479 \\
https://arxiv.org/pdf/2403.07786v4 \\
https://arxiv.org/pdf/2201.03898 \\
https://arxiv.org/pdf/1206.5538 \\
https://andor.oxinst.com/products/bc43- \\
https://freeflysystems.com/alta-x \\
https://visionaerial.com/switchblade-elite/ \\
https://www.eurac.edu/en/magazine/drones-for-science \\
https://www.math.utoronto.ca/ivrii/PDE-textbook/PDE-textbook.pdf \\
https://users.math.msu.edu/users/gnagy/teaching/ode.pdf \\
https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf \\
https://arxiv.org/pdf/2012.01399 \\
https://arxiv.org/pdf/1901.00137 \\

\noindent\makebox[\linewidth]{\rule{\paperwidth}{2pt}}\\\\

\normalsize

\noindent \textbf{Although initialization might \noindent almost exclusively bring practical implementation in virtuality, are we not restricted to give speculation towards extension in deployment over reality. The below links give starting directions towards such an extension. There, however, is no bound to the extent of data-collection, and so is the extent of scale boundless.} Physics, Mathematical Physics, Mathematics. America is a nation of explorers and great universities, pursue DARPA too.\\

\noindent \textbf{Millenium Problems.} Sub-problems to point towards the final problem. \textbf{Final Problem.} Machines can assume many forms, and the configuration of hardware owes its origin to a sound architectural blueprint. The utility of physical substance is dictated by known interaction between components, and there are many configuration that an assembly of components can assume. Easily seen, trivial, configuration are deducible from plain inspection. Substance might, however assume many forms, and one can distinguish over levels of 'difficulty' through which a configuration is brought to rise, in a sense similar to the varying levels of nuance observable in machines of varying levels of complexity. Therefore, pass the trivial, and pursue the non-trivial. Mathematics lays bear the essence of configural architecture, the realization of which can be virtual of physical. Our interest lies in the creation of machines that immensely infer, and all of mathematics know more intertwinition than at first glance seems to exist. The interest of many lie in the solution of ClayMath's Millenium Prize Problems, and so does popular demand incline hither. We approach our creation by solving these problems, tracking all the intermediary mathematics and physics, and collecting all notions towards a complete definition of our first machine.

\textit{Much of the prose in the following serves to suggest and direct, and so are the sentences short, and do we much summarize.} \\

\noindent \textbf{REGARD CLAYMATH REFERENCE-LIST OF CLAYMATH MILLENIUM PROBLEM ESSAYS AS MATHEMATICAL CURRICIULUM.} \textit{FIGURE OUT WAY OF GETTING AT ALL SOURCES.} Regard ClayMath Reference-list of ClayMath Millenium Problem Essays as Mathematical Curriculum.
\begin{center}
\textit{\textbf{--------- INQUIRIES OF THE MILLENIUM [CMI] ---------}}
\end{center}

\noindent \textit{
Clay Mathematical Essay Citation. Reliable sources towards multiple solutions of problems of notorious difficulty and importance. [An essay on the Hodge Conjecture in the Author's lingo would by the Author's momentary lacks in Modern Algebra be unfruitful. Instead, we cite from credible mathematicians. Mathematics $\iff$ Construction. The rest follows. We serve to bring substance to The Main Plan, and so shall we bring changes to The Main Plain as directed by Millenium Problems. And shall we bring mathematics as directed by evolving Main Plan. See .tex-file OutlineFullMain.tex]}
\begin{center}
\textbf{THE POINCAR CONJECTURE}
[JOHN MILNOR]
\end{center}

\noindent \textbf{Question.}
\textit{If a compact three-dimensional manifold $M^{3}$ has the property that every
simple closed curve within the manifold can be deformed continuously to a point, does it follow that $M^{3}$ is homeomorphic to the sphere $S^{3}$?}
\\

\noindent 
\textbf{Main Referential, John Milnor, The Poincar Conjecture, Clay Mathematics Institute, June 2004.}

\textit{"The topology of two-dimensional manifolds or surfaces was well understood in the 19th century. In fact there is a simple list of all possible smooth compact orientable surfaces...Hamilton was able to carry out this program and construct a metric of constant curvature, thus solving a very special case of the Elliptization Conjecture. However, in the general case, there are very serious difficulties, since this flow may tend toward singularities."}

\textbf{Mathematics-Curricular Reference List; Script-count 54.}
\begin{center}
\textbf{EXISTENCE AND SMOOTHNESS OF THE NAVIERSTOKES EQUATION
}
[CHARLES L. FEFFERMAN]
\end{center}

\noindent \textbf{Preliminaries.} For unknown velocity vector $u(x,t) = (u_{i}(x,t))_{1\leq i\leq n}$ and pressure $p(x,t)\in \mathbb{R}$ defined for position $x\in \mathbb{R}^{n}$ and time $t\geq0$, the Euler and Navier-Stokes Equations are to be solved. 
In CMI Essay on the problem, "Restrict Attention to incompressible fluid filling all of $\mathbb{R}^{n}$". The \textit{Navier-Stokes} equations are then given by

\begin{equation}
\frac{\partial}{\partial t}u_{i} + \sum_{j=1}^{n} u_{j}\frac{\partial u_{i}}{\partial x_{j}}
= \nu \Delta u_{i} + \frac{\partial p}{\partial x_{i}} + f_{i}(x,t) \\
\end{equation}
\begin{equation}
\text{div} u = \sum_{i=1}^{n} \frac{\partial u_{i}}{\partial x_{i}} = 0
\end{equation}
with initial conditions
\begin{equation}
u(x,0) = u^{\circ}(x) \,\,\,\,\, (x\in \mathbb{R}^{n})
\end{equation}
Here, $u^{\circ}(x)$ is a given, $c^{\infty}$ divergence-free vector field on $\mathbb{R}^{n}$, $f_{i}(x,t)$ are the components of a given, externally applied force (e.g. gravity), $\nu$ is a positive coefficient (the viscosity), and $\Delta = \sum_{i=1}^{n} \frac{\partial^{2}}{\partial x_{i}^{2}}$ is the Laplacian in the space variables. The \textit{Euler equations} are equations (0.1), (0.2), (0.3) with $\nu$ set equal to zero.

Equation (0.1) is just Newton's law $f=ma$ for a fluid subject to the external force $(f_{i}(x,t))_{1\leq i \leq n}$ and to the forces arising from pressure and friction. Equation (0.2) just says that the fluid is incompressible. For physically reasonable solutions, we want to to make $u(x,t)$ does not grow large as $|x| \rightarrow \infty$. Hence, we will restrict attention to force $f$ and initial conditions $u^{\circ}$ that satisfy
\begin{equation}
|\partial_{x}^{\alpha} u^{\circ}(x)| \leq C_{\alpha K} (1+|x|)^{-K} \,\,\,\,\,\, x \in \mathbb{R}^{n},\,\, \forall \alpha, K
\end{equation}
and
\begin{equation}
|\partial_{x}^{\alpha}   \partial_{t}^{m} f(x,t)| \leq C_{\alpha mK} (1+|x| + t)^{-K} \,\,\,\,\,\, (x,t) \in \mathbb{R}^{n} \times [0,\infty),\,\, \forall \alpha, m, K.
\end{equation}
We accept a solutions of (0.1), (0.2), (0.3) as physically reasonable only if it satisfies
\begin{equation}
p,u \in C^{\infty}(\mathbb{R}^{n} \times [0,\infty))
\end{equation}
and
\begin{equation}
\int_{\mathbb{R}^{n}} |u(x,t)|^{2} dx < C, \,\,\,\, \forall t\geq 0 \,\,\,\, (\text{bounded energy})
\end{equation}
Alternatively, to rule out problems at infinity we may look for spatially periodic solutions of (0.1), (0.2), (0.3). Thus, we assume that $u^{\circ}, f(x,t)$ satisfy
\begin{equation}
u^{\circ}(x+e_{j}) = u^{\circ}(x), \,\,\, f(x+e_{j}, t) = f(x,t) \,\,\, \text{for} 1\leq j \leq n
\end{equation}
$(e_{j} = j^{\text{th}})$ unit vector in $\mathbb{R}^{n}$.
In place of (0.4) and (0.5), we assume that $u^{\circ}$ is smooth and that

\begin{equation}
|\partial_{x}^{\alpha} \partial_{t}^{m} f(x,t)| \leq C_{\alpha m K}(1+ |t|)^{-K} \,\,\,\,\,\, (x,t) \in \mathbb{R}^{3} \times [0,\infty),\,\,\, 
\forall \alpha,m,K 
\end{equation}
We then accept a solutions (0.1), (0.2), (0.3) as physically reasonable if it satisfies 
\begin{equation}
u(x,t) = u(x+e_{j}, t)\,\,\,\,\, (x,t) \in \mathbb{R}^{3} \times [0,\infty) \,\,\,\, \text{for} \,\,\, 1\leq j \leq n
\end{equation}
and
\begin{equation}
p, u \in C^{\infty}(\mathbb{R}^{n} \times [0,\infty))
\end{equation}












A fundamental problem in analysis is to decide whether such smooth, physically reasonable solutions exists for the Navier-Stokes equations. To give reasonable leeway to solvers while retaining the heart of the problem, we ask for a proof of one of the following four statements.


\textbf{(A) Existence and smoothness of Navier-Stokes solutions on $\mathbb{R}^{3}$}. Take $\nu > 0$ and $n=3$. Let $u^{\circ}(x)$ be any smooth , divergence-free vector field satisfying (0.4). Take $f(x,t)$ to be identically zero. Then there exist smooth functions $p(x,t), u_{i}(x,t)$ on $\mathbb{R}^{3} \times [0,\infty)$ that satisfy (0.1), (0.2), (0.3), (0.6), (0.7).

\textbf{(B)Existence and smoothness of Navier-Stokes solutions in  $\mathbb{R}^{3}/ \mathbb{Z}^{3}$}. Take $\nu >0$ and $n=3$. Let $u^{\circ}(x)$ be any smooth, divergence-free vector field satisfying (0.8); we take $f(x,t)$ to e identically zero. Then there exist smooth functions $p(x,t)$, $u_{i}(x,t)$ on $\mathbb{R}^{3}\times [0,\infty)$ that satisfy (0.1), (0.2), (0.3), (0.4), (0.10), (0.11).



\textbf{(C) Breakdown of Navier-Stokes solutions on} $\mathbb{R}^{3}$ Take $\nu > 0$  and $n=3$. Then there exist a smooth, divergence-free vector field $u^{\circ}(x)$ on $\mathbb{R}^{3}$ and a smooth $f(x,t)$ on $\mathbb{R}^{3} \times [0,\infty)$, satisfying (0.4), (0.5), for which there exist no solutions $(p,u)$ of (0.1), (0.2), (0.3), (0.6), (0.7) on $\mathbb{R}^{3} \times [0,\infty)$.


\textbf{(D) Breakdown of Navier-Stokes solutions on.} $\mathbb{R}^{3}/\mathbb{Z}^{3}$ Take $\nu > 0$  and $n=3$. Then there exist
a smooth, divergence-free vector field $u^{\circ}(x)$ on $\mathbb{R}^{3}$ and a smooth $f(x,t)$ on $\mathbb{R}^{3} \times [0,\infty)$, satisfying (0.8), (0.9), for which there exist no solutions $(p,u)$ of (0.1), (0.2), (0.3), (0.10), (0.11) on $\mathbb{R}^{3} \times [0,\infty)$. \\






\noindent \textbf{Main Referential: Clay Mathematics Institute.} \textbf{Charles L, Fefferman, Existence and Smoothness of the Navier-Stokes Equation, Clay Mathematics Institute [Date Unknown]}.

\textit{"The topology of two-dimensional manifolds or surfaces was well understood in the 19th century. In fact there is a simple list of all possible smooth compact orientable surfaces...Hamilton was able to carry out this program and construct a metric of constant curvature, thus solving a very special case of the Elliptization Conjecture. However, in the general case, there are very serious difficulties, since this flow may tend toward singularities."}

\textbf{Mathematics-Curricular Reference List; Script-count 9.}
\begin{center}
\textbf{YANG-MILLS EXISTENCE AND MASS GAP}
[ARTHUR JAFFE AND EDWARD WITTEN]
\end{center}

\noindent \textbf{YangMills Existence and Mass Gap.} 
\textit{Prove that for any compact simple gauge group G, a non-trivial quantum Yang-Mills theory exists on $\mathbb{R}^{4}$ and has a mass gap $\Delta>0$. Existence includes establishing axiomatic properties at as strong as those cited in [K. Osterwalder and R. Schrader, Axioms for Euclidean Greens functions, Comm. Math. Phys. 31 (1973), 83112, and Comm. Math. Phys. 42 (1975), 281-305] and [R. Streater and A. Wightman, PCT, Spin and Statistics and all That, W. A. Benjamin, New York, 1964.].} \\

\noindent \textbf{Main Referential: Quantum Yang-Mills Theory}
Arthur Jaffe, Edward Witten. [Data Unknown]

\textit{"Since the early part of the 20th century, it has been understood that the description of nature at the subatomic scale requires quantum mechanics. In quantum mechanics, the position and velocity of a particle are noncommuting operators acting on a Hilbert space, and classical notions such as the trajectory of a particle do not apply...While this work in three dimensions is important in its own right, a qualitative breakthrough came with Balabans extension of this analysis to four dimensions [Source Bibliography Number 3]. This includes an analysis of asymptotic freedom to control the renormalization group flow as well as obtaining quantitative estimates on effects arising from large values of the gauge field."}

\textbf{Mathematics-Curricular Reference List; Script-count 50.}
\begin{center}
\textbf{THE P VERSUS NP PROBLEM} [STEPHEN COOK]
\end{center}
\noindent \textbf{Preliminaries.} To define the problem precisely it is necessary to give a formal model of a computer. The standard computer model in computability theory is the Turing machine, introduced by Alan Turing in 1936. We follow standard practice and define the class $\mathbf{P}$ in terms of Turing machines. Formally the elements of the class $\mathbf{P}$ are languages.  Formally the elements of the class $\mathbf{P}$ are languages. Let $\Sigma$ be finite alphabet (that is, a finite nonempty) with at least two elements, and let $\Sigma^{*}$ be the set of finite strings over $\Sigma$. Then a $\textit{language over}$ $\Sigma$ is a subset $L$ of $\Sigma^{*}$. Each Turing machine $M$ has an associated $\textit{input alphabet}$ $\Sigma$. For each string $w$ in $\Sigma^{*}$ there is a computation associated with M with input $w$. (The notions of Turing machine and computation are defined formally in the appendix). We say that $M$ \textit{accepts} w if this computation terminates in the accepting state. Note that $M$ fails to terminate. The \textit{language accepted by M}, denoted $L(M)$, has associated alphabet $\Sigma$ and is defined by 
\begin{equation*}
L(M) = \{  w \in \Sigma^{*}| M \,\,\text{accepts}\,\, w  \}.
\end{equation*}
We denote by $t_{M}(w)$ the number of steps in the computation of $M$ on input $w$. If this computation never halts, then $t_{M}(w) = \infty$. For $n\in \mathbb{N}$ we denote by $T_{M}(n)$ the \textit{worst case run time} of $M$; that is,
\begin{equation*}
T_{M}(n) = \text{max}\{ t_{M}(w) | w\in \Sigma^{n} \},
\end{equation*}

\noindent where $\Sigma^{n}$ is the set of all strings over $\Sigma$ of length $n$. We sau $M$ \textit{runs in polynomial time} if there exists $k$ such that for all $n$, $T_{M}(n) \leq n^{k} +k$. Now we define the class $\mathbf{P}$ of languages by $\mathbf{P}$
\begin{equation*}
\mathbf{P} = \{  L|L=L(M) \,\,\,\text{for some Turing machine M that runs in polynomial time}   \}.
\end{equation*}
The notation $\mathbf{NP}$ stands for "nondeterministic polynomial time", since originally $\mathbf{NP}$ was defined in terms of nondeterministic machines (that is, machines, that have more than one possible move from a given configuration). However, now it is customary to give an equivalent definition using the notion of a \textit{checking relation}, which is simply a binary relation $R\subset \Sigma^{*} \times \Sigma_{1}^{*}$ for some finite alphabets $\Sigma$ and $\Sigma_{1}$. We associate with each such relation $R$ a language $L_{R}$ over $\Sigma \cup \Sigma_{1} \cup \{ \# \}$ defined by

\begin{equation*}
L_{R} = \{ w \# y | R(w,y)   \}
\end{equation*}

where the symbol is not in $\Sigma$. We say that $R$ is \textit{polynomial-time} iff $L_{R} \in \mathbf{P}$. Now we define the calss $\mathbf{NP}$ iff there is $k\in \mathbb{N}$ and a polynomial-time checking relation $R$ such that for all $w\in \Sigma^{*}$,
\begin{equation*}
w\in L \iff \exists y(|y| \leq |w|^{k} \text{and} R(w,y)),
\end{equation*}
\noindent where $|w|$ and $|y|$ denote the lengths of $w$ and $y$, respectively.
\textbf{Problem Statement.} Does \textbf{P= NP}? \\


\noindent \textbf{Main Referential: Clay Mathematics Institute. Stephen Cook, The P versus NP problem, Clay Mathematics Institute [Date Unknown].}

\textit{
"The $\mathbf{P}$ versus $\mathbf{NP}$ problem is to determine whether every language accepted by some nondeterministic algorithm in polynomial time is also accepted by some (deterministic) algorithm in polynomial time. To define the problem precise it is necessary to give a formal model of a computer...If the computation is finite, then the number of steps is one less than the number of configurations; otherwise the number of steps is infinite. We say that $M$ \textit{accepts w} iff the computation is finite and the final configuration contains the state $q_{\text{accept}}$."
}

\textbf{Mathematics-Curricular Reference List; Script-count 38.}
\begin{center}
\textbf{THE HODGE CONJECTURE} 
[PIERRE DELIGNE]
\end{center}

\noindent \textbf{Preliminaries.} Clay Mathematical Essay Citation. Reliable source towards multiple solutions of problems of notorious difficulty and importance. Script can be informative, suggestive, and directive.

A pseudo-complex structure on a $C^{\infty}$-manifold $X$ of dimension $2N$ is a $\mathbb{C}$-module structure on the structure on the tangent bundle $T_{X}$. Assume $X$ complex analytic. (Recall UvA Course Analysis 4.) A $(p,q)$-form is then a form which, in local holomorphic coordinates, can be written as
\begin{equation*}
\sum   a_{i_{1},\dots, i_{p}, j_{1}\dots j_{q}} dz_{i_{1}} \wedge \dots
\wedge z_{i_{p}} \wedge \bar{z}_{j_[1]} \wedge \dots \wedge d\bar{z}_{j_{q}}
\end{equation*}
and the decomposition $\Omega = \oplus \Omega^{p,q}$ induces a decomposition $d=d^{'} + d^{''}$ of the exterior differential, with $d'$ (resp. d'') of degree (1,0) (resp. (0,1)). \\

\noindent The space $H^{n}(X,\mathbb{C})$ is the space of closed $n$-forms modulo exact forms, and if we define $H^{p,q}$ to be the space of closed ($p,q$)-forms modulo the $d'd''$ of $(p-1,q-1)$-forms, the natural map
\begin{equation*}
\oplus_{p+q=n} H^{p,q} \rightarrow H^{n}(X,\mathbb{C})
\end{equation*}
is an isomorphism. If we choose a Khler structure on $X$, one can give the following interpretation to the decomposition (1) of $H^{n}(X,\mathbb{C})$: the action $\mathbb{C}^{*}$ on forms commutes with the Laplace operator, hence induces an action of $\mathbb{C}^{*}$ on the space $\mathcal{H}^{n}$ of harmonic $n$-forms. We have $\mathcal{H}^{n} \overset{\sim}{\rightarrow} H^{n}(X,\mathcal{C})$ and $H^{p,q}$ identifies with the space of harmonic $(p,q)$-forms. \textbf{Hodge filtration.} When $X$ moves in a holomorphic family, the \textit{Hodge filtration} $F^{p} := \oplus_{a\geq p} H^{a,n-a}$ of $H^{n}(X,\mathbb{C})$ is better behaved than the Hodge decomposition. Locally on the parameter space $T, H^{n}(X_{t},\mathbb{C})$ is independent of $t\in T$ and the Hodg filtration can be viewd a s a variable filtration $F(t)$ on a fixed vectro space. It varies holomorphically with $t$, and obeys Griffiths transveersality: at first order around $t_{0} \in T$, $F^{p}(t)$ remains in $F^{p-1}(t_{0})$. \textbf{Hodge Classes.} So far, we have computed cohomology using $C^{\infty}$ forms. We could as well have used forms with generalized functions coefficients, that is, currents. The resulting groups $H^{n}(X,\mathbb{C})$ and $H^{p,q}$ are the same. If $Z$ is a closed analytic subspace of $X$, of complex codimension $p$, $Z$ is an integral cycle and, by Poincar duality, defines a class \text{cl}(Z) in $H^{2p} (X, \mathbb{Z})$. The integration current on $Z$ is a closed $(p,p)-$form with generalized function coefficients, representing the image of $\text{cl}(Z)$ in $H^{2p}(X,\mathbb{C})$. The class $\text{cl}(Z)$ in $H^{2p}(X,\mathbb{Z})$ is hence of type $(p,p)$ in the sense that its image in $H^{2p}(X,\mathbb{C})$ is. rational $(p,p)$-classes are called Hodge classes. They form in the group
\begin{equation*}
H^{2p}(X,\mathbb{Q}) \cap   H^{p,p}(X) =
H^{2p}(X,\mathbb{Q}) \cap F^{p} \subset H^{2p}()X, \mathbb{C}.
\end{equation*}
\noindent \textbf{Hodge Conjecture}. On a projective non-singular algebraic variety over $\mathbb{C}$, any Hodge class is a rational linear combination of classes \text{cl}(Z) of algebraic cycles. \\




\noindent \textbf{Main Referential: Clay Mathematics Institute. Pierre Deligne, The Hodge Conjecture [Date Unknown].}
[ANDREW WILES]

\textit{
We recall that a pseudo complex structure on a $C^{\infty}$-manifold $X$ of dimension $2N$ is a $\mathbb{C}$-module structure on the tangent bundle $T_{X}$. Such a module structure induces an action of the group $\mathbb{C}^{*}$ on $T_{X}$, with $\lambda \in \mathbb{C}^{*}$ acting by multiplication by $\lambda$...If $\mathfrak{q}_{1}$ and $\mathfrak{q}_{2}$ were $\text{cl}(Z_{1})$ and  $\text{cl}(Z_{2})$, $Z_{1}$ and $Z_{2}$ could be chosen to be defined over $\bar{\mathbb{Q}}$ and $\kappa$ would be the intersection number of the reductions $Z_{1}$ and $Z_{2}$. Same question for the intersection number of the reduction of $\mathfrak{q}_{1}$, over $\mathbb{F}$ with the class of an algebraic cycle on $A$.}

\textbf{Mathematics-Curricular Reference List; Script-count 11.}
\begin{center}
\textbf{THE BIRCH AND SWINNERTON-DYER CONJECTURE}
[ANDREW WILES]
\end{center}

\noindent \textbf{Preliminaries.} If the genus of $C_{0}$ is greater than or equal to 2, then $C_{0}(\mathbb{Q})$ is finite. If $C$ is an elliptic curve over $\mathbb{Q}$, then
\begin{equation*}
    C(\mathbb{Q}) \simeq \mathbb{Z}^{r} \oplus C(\mathbb{Q})^{\text{tors}}
\end{equation*}
for some integer $r\geq 0$, where $C(\mathbb{Q})^{\text{tors}}$ is finite abelian group. The integer $r\geq 0$, where $C(\mathbb{Q})^{tors}$ is finite abelian group.  The integer $r$ is called the rank of $C$. It is zero if and only ig $C(\mathbb{Q})$ is finite. We can find an affine model for the curve in Weierstrass form
\begin{equation*}
C: y^{2 } = x^{3}+ac+b,
\end{equation*}
with $a,b\in \mathbb{Z}$. We let $\Delta$ denote the discriminant of the cubic and set
\begin{align*}
N_{p} &:= \#\{\text{solutions of} \,\,\,y^{2} \equiv x^{3} +ax +b  \,\,\,\text{mod} \,\,\,p\} \\
a_{p} &:= p-N_{p}.
\end{align*}
Then we can define the incomplete $L$-series of $C$ (incomplete because we omit the Euler factors for primes $p|2\Delta$) by
\begin{equation*}
L(C,s):= \prod_{p|2\Delta}(1-a_{p} p^{-s} + p^{1-2s})^{-1}.
\end{equation*}
We view this as a function of the complex variable $s$ and this Euler product is then known to converge for Re(s) 3/2. A conjecture going to Hasse predicted that $L(C,s)$ should have a homomorphic continuation as a function of $s$ to the whole complex plane. This has now been proved. We can now state the millenium prize problem. \\

\noindent \textbf{Conjecture} (Birch and Swinnerton-Dyer). The Taylor expansion of $L(C,s)$ at $s=1$ has the form
\begin{equation*}
L(C,s) = c(s-1)^{r} + \textit{higher order terms}
\end{equation*}
with $c\neq 0$ and $r = \text{rank}(C(\mathbb{Q}))$. \\


\noindent \textbf{Main Referential: Andrew Wiles, The Birch and Swinnerton-Dyer Conjecture, Clay Mathematics Institute, Clay Mathematics Institute [Date Unknown].}

\textit{
We recall that a pseudo complex structure on a $C^{\infty}$-manifold X of dimension $2N$ is a $\mathbb{C}$-module structure on the tangent bundle $T_{X}$. Such a module structure induces an action of the group $\mathbb{C}^{*}$ on $T_{X}$, with $\lambda \in \mathbb{C}^{*}$ acting by multiplication by $\lambda$...In conclusion, although there has been some success in the last fifty years in limiting the number of rational points on varieties, there are still almost no methods for finding such points. It is hoped that a proof of the Birch and Swinnerton-Dyer conjecture will give some insight concerning this general problem.}

\textbf{Mathematics-Curricular Reference List; Script-count 27.}

\begin{center}
\textbf{OMNI-OBSERVANT ARTIFICIAL GENERAL SUPERINTELLIGENCE} [ALI NAJIB]
\end{center}
\textbf{Last problem.} \textit{Project Objective.}  Here follows a continuation and query of generalizations of the above Outline [Architectural Designs for General Intelligence]. See Introduction Millenium Problems for a first motivation. See file-bibliography for one bibliography. Utilize all millenium-referentials for further bibliographies. \textbf{Machines in software and hardware follow}. Hints towards new notions follow. Mind the existence of theories in Artificial Intelligence. \textbf{Query: Build the mathematics at a level that transcends those of the solutions to the millenium-problems.}







\newpage

\noindent \textbf{SUPPLEMENTARY REFERENTIAL. REFERENTIAL SCRIPTS.\\ Script-Count 30.}
\textit{A list of textbooks to serve as a Student's Introduction to Mathematics. This Supplementary Referential is meant to be Read Fast. Acquire Analogue Textbook if Digital Textbook unavailable. Digital Scripts can be ArXiv entries or independent files.} \\

\noindent \textbf{Functional Analysis and Differential Equations.} \textit{'ODE': An Introduction to Dynamical Systems; Continuous and Discrete, Second Edition. R. Clark Robinson. 'Introductory' PDE: Introduction to Partial Differential Equations. 'Advanced' PDE:  Functional Analysis, Sobolev Spaces, and Partial Differential Equations by Haim Brezis. [Recommender Comment.] This violates your rule of not developing the functional analysis material, but is a very good book. You can skip the stuff you know and jump right to the PDE/operator bits. An Introduction to Partial Differential Equations by Michael Renardy and Robert Rogers. Here you want the last part of the book, say after chapter 8. There's a lot of nice stuff in Chapters 10-12 that uses lots of functional analysis to solve nonlinear elliptic problems, etc.} \\

\noindent \textbf{Complex Analysis.} Several Complex Variables, Jaap Korevaar, Jan Wiegerinck; Harmonic and Complex Analysis in Several Variables \textit{[Acquire Analogue Textbook]}. \\

\noindent \textbf{Quantum Field Theory, Quantum Yang-Mills Theory.}
\textit{Gauge Theory, Ch2: Yang-Mills Theory. 
Quantum Fields and Strings: A Course for Mathematicians. 
Quantum Yang-Mills Theory in two-dimensions.
Aspects of Symmetry, Sidney Coleman.} \\
 
\noindent \textbf{Algebraic Topology.}
\textit{J. Munkres, Topology: A First Course, PrenticeHall, Englewood Cliffs, NJ, 1975. 
E.E. Moise, Geometric Topology in Dimensions 2 and 3, Springer, New York, 1977. } \\

\noindent \textbf{Theory of Theories.}
\textit{PCT, Spin and Statistics, and All That; Raymond F. Streater and Arthur S. Wightman. 
Axioms for Euclidean Green's Functions II; Konrad Osterwalder*, Robert Schrader.} \\

\noindent \textbf{Analysis on Manifolds.} \textit{Introductory. Analysis on Manifolds, Munkres (covers Differential Forms),
(Analogue textbook available in domestic library.)
Advanced. Introduction to Smooth Manifolds,
Advanced. A Geometric Approach to Differential Forms. 
Functional analysis, PDE's (Functional Analysis, Sobolev Spaces, and Partial Differential Equations by Haim Brezis.), Monotone Operators in Banach Space and Nonlinear PDE.} \\

\noindent \textbf{Fourier Analysis.}
\textit{Fourier Analysis, Classical Fourier Analysis; Modern Fourier Analysis [Acquire Analogue Textbook]. } \\

\noindent \textbf{Quantum Field Theory.} \textit{The Physics of Quantum Mechanics, James Binney, David Skinner. 
Quantum Fields and Strings: A Course for Mathematicians. 
Quantum Yang-Mills Theory in Two Dimensions Exact versus Perturbative, Timothy Nguyen. 
Gauge Theory, Ch2: Yang-Mills Theory. 
[Gauge Theory] Aspects of Symmetry, Sidney Coleman.} \\

\noindent \textbf{Emerging algebra's more generally.}
\textit{Modern Algebra An Introduction. Basic Algebra 1, Jacobson, 
Basic Algebra 2, Jacobson, Advanced Modern Algebra, Rotman}. \\

\noindent \textbf{Theory of computation.}
\textit{M. Sipser, Introduction to the Theory of Computation, PWS Publ., Boston, 1997.
T. Cormen, C. Leiserson, R. Rivest, and C. Stein, Introduction to Algorithms, 2nd edition, McGraw Hill, New York, 2001. 
J. Krajicek, Bounded Arithmetic, Propositional Logic, and Complexity Theory, Cambridge University Press, Cambridge, 1995.} \\

\noindent \textbf{Number Theory.}
\textit{A. Weil, Basic Number Theory, Birkha-user, Boston, 1984, 
S. Lang, Number Theory III, Encyclopdia of Mathematical Sciences, vol. 60, SpringerVerlag, Heidelberg, 1991.} 



\begin{center}
\noindent \textbf{SCRIPT ACCESS AND CONNECTION TO OUTLINE}
\end{center}

\noindent \textbf{Script Access}
\textit{See .tex-file ScriptAccess.tex.} \\

\noindent \textbf{Extension of Outline}
\textit{See .tex-file OutlineFullMain.tex.} \\













\begin{center}
\noindent \textbf{MOMENTARY ORDER OF SCRIPT-COVERAGE}
\end{center}


\noindent \textbf{Momentary Order of Script-Coverage. (Read fast, solve fast, track solutions. Towards definitions theme General Intelligence over General Realms.)} 
Artificial General Super Intelligence I (Marcus Hutters) $\rightarrow$
(Dynamical Systems) ODE I $\rightarrow$ PDE I $\rightarrow$  PDE II (Functional Analytic) $\rightarrow$  Several Complex Variables $\rightarrow$
Harmonic and Complex Analysis in Several Variables 
$\rightarrow$ Topology: A First Course
$\rightarrow$ Geometric Topology in Dimensions 2 and 3
$\rightarrow$ 
Analysis on Manifolds,
$\rightarrow$
Introduction to Smooth Manifolds
$\rightarrow$
A Geometric Approach to Differential
Forms
$\rightarrow$
Classical Fourier Analysis
$\rightarrow$
Modern Fourier Analysis
$\rightarrow$
Modern Algebra An Introduction
$\rightarrow$
Basic Algebra 1
$\rightarrow$
Basic Algebra 2
($\rightarrow$
Introductory Modern Algebra)
$\rightarrow$
Advanced Modern Algebra
$\rightarrow$
The Physics of Quantum Mechanics, James Binney
$\rightarrow$
David Skinner. Quantum Fields and Strings: A Course for Mathematicians. 
$\rightarrow$
Quantum Yang-Mills Theory in Two Dimensions Exact versus Perturbative, Timothy
Nguyen
$\rightarrow$ Gauge Theory, Ch2: Yang-Mills Theory
$\rightarrow$ [Gauge Theory] Aspects of Symmetry, Sidney Coleman
$\rightarrow$
Introduction to Algorithms
$\rightarrow$
Bounded Arithmetic, Propositional Logic, and Complexity Theory
$\rightarrow$
(Explore the literature on Number Theory)
Basic Number Theory
$\rightarrow$
Number Theory III.
$\rightarrow$
Millenials
$\rightarrow$
Omni-Observant Artificial General Super Intelligence I \textit{(towards project Objective)}
\textbf{Extend Script-Coverage by picking from Bibliographies Millenium-Referentials.}

\newpage

\nocite{*}
\printbibliography[heading=bibintoc]

\end{document}